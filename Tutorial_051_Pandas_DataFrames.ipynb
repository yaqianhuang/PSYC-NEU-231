{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 05, Part 1: Pandas DataFrames \n",
    "[The official project homepage](https://pandas.pydata.org)\n",
    "\n",
    "* Goal\n",
    "    * Extend what we learned about Series objects in the previous tutorial to their 2D counterpart - DataFrames\n",
    "    * Develop some tools for dealing with missing data (not exhaustive, but a good start)\n",
    "    * Take this chance to also learn a bit about file input/output (I/O) and some other more advanced coding techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames\n",
    "\n",
    "[Pandas quick start guide for DataFrames](https://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe)\n",
    "\n",
    "* A DataFrame (DF) is a labeled data struture that can be thought of as a 2D extension of the Series objects that we discussed in the first part of the tutorial\n",
    "* A DF can accept many types of input, from a 2D ndarray, multiple Series, a dict of 1D arrays, another DF, etc\n",
    "* Like a Series, DFs contain data values and their labels. Because we're now dealing with a 2D structure, we call the **row labels the index argument** and the **column labels the column argument**. \n",
    "    * Like a Series, if you don't explicitly assign row and column labels, then they will be auto-generated (but not as useful as specifying the labels yourself!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Much of what we learned about Series objects will generalize to DFs, so here we'll focus on some of key functionality that might not be obvious based on the previous tutorial.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "One more quick note: if using an older version of Python (earlier than 3.6) and Pandas (earlier than 0.23) and you create a DF from a dict without explicitly specifying column names, then the column names will be entered into the DF based on lexical order\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard numpy module\n",
    "import numpy as np\n",
    "\n",
    "# import a generic pandas object and also a few specific functions that we'll use\n",
    "import pandas as pd \n",
    "from pandas import DataFrame, read_csv\n",
    "\n",
    "# new - get and store current file path for file i/o later on in tutorial\n",
    "import os\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make up a data set to demonstrate functionality, will import some real data later on\n",
    "* Here we'll pretend that we did a unit recording experiment \n",
    "    * There are two stimulus conditions\n",
    "    * And we are recording from 10 different neurons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed random number generator so that we're all seeing the same thing in class\n",
    "np.random.RandomState(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index lables for our 10 neurons...see previous tutorial for more elegant ways of generating\n",
    "# index labels, here we're just going to write them out\n",
    "neuron_labels = ['Nrn0', 'Nrn1','Nrn2','Nrn3','Nrn4','Nrn5','Nrn6','Nrn7','Nrn8','Nrn9']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate response in each neuron to stimulus 1...\n",
    "resp1_hz = [14, 27, 62, 88, 45, 56, 75, 63, 33, 46]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a response to stimulus 2...use random.randint just for practice/fun\n",
    "min_resp = 0  # inclusive\n",
    "max_resp = 90 # exclusive\n",
    "resp2_hz = np.random.randint(min_resp, max_resp, len(resp1_hz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New - use 'zip' function to wrap up the data from each list into one list\n",
    "[reference page for zip](https://www.w3schools.com/python/ref_func_zip.asp)\n",
    "\n",
    "* Operates just like it sounds  - takes a set of iterators and groups them together into a single iterator with the 1st element in the resultant iterator comprised of the first element of each iterator 'zipped' together, then the second element from each iterator zipped together, etc. \n",
    "* Length of resulting iterator limited by the length of the shortest input iterator!\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "Because the length of the resulting iterator is limited by length of shortest input iterator, you can sometimes not get an error if you try to zip together iterators with unequal lengths - this is fine if intenitonal, but if the unequal length was caused by a bug, then you may not find it when using zip!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_data = list(zip(resp1_hz, resp2_hz))\n",
    "print(neuron_data)\n",
    "\n",
    "print('Grab one index to see the two response arrays zipped together:')\n",
    "print(neuron_data[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: to unzip go like this and you'll get back the original\n",
    "uz_data1, uz_data2 = *neuron_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a DataFrame object to hold the contents of the data set\n",
    "[DataFrame help page](https://pandas.pydata.org/pandas-docs/version/0.21/generated/pandas.DataFrame.html)\n",
    "\n",
    "* Just like with the pd.Series call, you specify the data, index labels (row labels in this case)\n",
    "* In addition to row labels, you can also specify column labels (with 'columns')\n",
    "* Can also specify data type (default is inferred)\n",
    "* Like pd.Series you can ask for an independent copy of the data (copy=True) or you will get a view by default (i.e. copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the call to pd.DataFrames to create the DF - usage much like pd.Series\n",
    "df = pd.DataFrame(data = neuron_data, index=neuron_labels, columns = ['resp1', 'resp2'])\n",
    "\n",
    "# take a look at the output...\n",
    "display(df)   # compare to print(df) - looks nicer with display thanks to iPython backend "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another handy display function...good for large dfs that are too big to fit - at least you can get an idea of \n",
    "# the overall structure\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a high-level summary of the data using built-in functionality of DataFrame object\n",
    "[API reference page](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first call this using the defaults\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can specify different values to change behavior...\n",
    "df.describe(percentiles=np.linspace(0,1,11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "BEGIN SIDEBAR - Common source of NaNs in data and how to deal with them in general\n",
    "\n",
    "Important bit of info for avoiding a common source of confusion (and potential bugs!!!)\n",
    "\n",
    "\n",
    "* Note that if you make a DF out of a set of Series (e.g. a dict of Series), then the resulting DF index labels will be the union of the index labels in each Series\n",
    "* This can be confusing because the DF will still be formed even if you have mismatching labels or even if you have two series of different sizes..\n",
    "* Fortunately, the misaligned (or missing) values will be filled in with NaNs ('Not-a-Number') to serve as a placeholder for the misaligned or missing info\n",
    "* Quick demo below before continuing on with our sample neuron data from above \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a set of two Series with unequal lengths stored in a dict, \n",
    "# with each Series having data and index labels\n",
    "\n",
    "# Note that Series 1 has 4 elements, but Series 2 has 5 elements!\n",
    "\n",
    "data_dict = {'dict0' : pd.Series(data = np.random.randn(4), index=['0','1','2','3']), \n",
    "            'dict1' : pd.Series(data = np.random.randint(0,5,5), index=['0','1','2','3','4'])}\n",
    "\n",
    "# make a data frame\n",
    "weird_df = pd.DataFrame(data_dict)\n",
    "\n",
    "# take a look - notice that pd.DataFrame did not throw an error even though\n",
    "# the input Series are different sizes...however, it did mark the missing value \n",
    "# with a NaN\n",
    "display(weird_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\">\n",
    "Because of the above behavior, often good to frequently check for NaNs in your data to identify processing steps that might have gone awry...(unless you are expecting NaNs as part of routine processing, in which case this might not be very helpful). \n",
    "Many ways to do this, but here is one way that works pretty well with identifying the presence of a NaN anywhere in the DataFrame (which is handy beacuse often times the DataFrames are too large to easily see in their entirety) \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show True/False for each element in DF\n",
    "# first do it the NumPy way\n",
    "display(np.isnan(weird_df))\n",
    "\n",
    "# use the 'any' method to figure out if any entries are NaN\n",
    "if np.isnan(weird_df).any:\n",
    "    print('weird...you have NaNs in your data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas also provides functions for dealing with NaNs (or missing values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return true for NaNs\n",
    "pd.isna(weird_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or return true for non NaNs\n",
    "pd.notna(weird_df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to just one column at a time\n",
    "# note that you can call the function from the object\n",
    "# directly (instead of using the pd object)\n",
    "weird_df['dict0'].notna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate across missing values...\n",
    "* Default is linear, but can do all sorts of other interpolations using scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_df = weird_df.interpolate()\n",
    "display(weird_df)\n",
    "display(norm_df)\n",
    "# note that since the missing value is the last in the series\n",
    "# it just takes the value of the 2nd to last entry\n",
    "# otherwise it would be linearly interpolated by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can also fill NaNs lots of other ways...like assigning the mean of all or some of the data to NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g. fill the NaN with the mean of the first column!\n",
    "weird_df.fillna(weird_df.mean()['dict0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\">\n",
    "\n",
    "This is just a small subset of methods for finding and dealing with missing data. See this link for a more complete set of possibilities:\n",
    "\n",
    "[Pandas missing data](https://pandas.pydata.org/pandas-docs/stable/missing_data.html)\n",
    "\n",
    "Be careful rooting out NaNs using equivalence testing - NaNs don't compare equal (cause they are missing or unknown values). Better to use boolean indexing based on the return vector from np.isnan or pd.isna, etc\n",
    "\n",
    "\n",
    "Remember that in most cases:\n",
    "* When summing data, NaNs will be treated as zero.\n",
    "* Functions like cumsum() and cumprod() ignore NaNs by default, but will propogate them to returned arrays. \n",
    "    * To change behaviour to include NaNs, often use skipna=False or similar.\n",
    "* If multiplying by NaN, the result is generally NaN\n",
    "    * Except if computing the prod of all NaNs, in which case the answer is 1!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an array of nans\n",
    "x = np.repeat(np.nan, 10)\n",
    "print(x[0]*x[1])  # NaN\n",
    "\n",
    "# convert x to a series\n",
    "s = pd.Series(x)\n",
    "# compute the prod of elements in series\n",
    "pd.Series.prod(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "END SIDEBAR...now back to the main thread and our 10 neuron experimental data\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing, adding, deleting entire columns\n",
    "* Think of the DF as a dict of Series objects with common labels - much of the syntax is the same as for dicts (and for Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the second column from our DF\n",
    "display(df['resp2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a column is easy and can be done dynamically (on the fly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a third response column as the product of the first two columns\n",
    "df['resp3'] = df.resp1 * df.resp2\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing columns is also easy and done on the fly... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the del command will delete a column from the DF\n",
    "# note that here you have to use the df['resp3'] notation\n",
    "# the df.resp3 notation will not work.\n",
    "del df['resp3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instead of deleting outright, you can also \"pop\" a column out and asign it to another variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a third response column as the product of the first two columns\n",
    "df['resp3'] = df.resp1 * df.resp2\n",
    "\n",
    "# then pop it out\n",
    "resp3 = df.pop('resp3')\n",
    "display(resp3)\n",
    "\n",
    "# now df is back down to just 2 columns\n",
    "print('\\n')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on indexing and selection of specific coordinates in a DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row selection - this is a bit more confusing as there are many methods\n",
    "* You can use df.loc to select a row by its label name\n",
    "* You can use df.iloc to select a row by its integer location \n",
    "* You can use boolean vectors to select a set of rows that satisfy some condition\n",
    "* You can slice rows using standard notation e.g. df[1:3] for rows 1-3\n",
    "* You can also isolate a particular row/column using a combo of column indexing (see above) and standard slicing notation\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "Contrary to usual slicing conventions, both the start and the stop indices are included when using the DF.LOC function...see below for demo. This makes sense (sort of) because you're indexing by label name, not by a zero-based integer index. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data from 2nd neuron across both stimulus conditions\n",
    "df.loc['Nrn1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAREFUL!\n",
    "# data from 2nd-6th neuron inclusive across both stimulus conditions\n",
    "df.loc['Nrn1':'Nrn5']\n",
    "\n",
    "# again, just need to be careful but this makes sense given that you're indexing based on label name (not 0-based counting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data from 5th neuron across both stimulus conditions\n",
    "df.iloc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data from 2nd-5th neuron across both stimulus conditions\n",
    "df[2:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can use the trick for returning only a subset of values from a function that we discussed in\n",
    "# the randomization/bootstrapping lecture:\n",
    "# here grab the 2nd entry from the 2nd column\n",
    "print('2nd column, 2nd entry')\n",
    "print(df['resp2'][1])\n",
    "\n",
    "# can also go like this\n",
    "print('2nd column, 4th entry')\n",
    "print(df.resp2[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data alignment - like with the Series object, but accounts for a union of both column and index(row) labels\n",
    "* Recall from the tutorial on Series objects that if you have mismatched or different length Series, you can still combine them and do operations on them.\n",
    "* The missing/mismatched elements in the resulting Series was marked with a NaN, and the same will happen with a DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make two dfs and then combine them...\n",
    "\n",
    "# first df will be 3 x 4\n",
    "r0 = 3\n",
    "c0 = 4\n",
    "\n",
    "df0 = pd.DataFrame(np.random.rand(r0,c0), index=['r1', 'r2', 'r3'], columns=['a', 'b', 'c', 'd'])\n",
    "\n",
    "# second df will be 4 x 5\n",
    "r1 = 4\n",
    "c1 = 5\n",
    "\n",
    "df1 = pd.DataFrame(np.random.rand(r1,c1), index=['r1', 'r2', 'r3','r4'], columns=['a', 'b', 'c', 'd','e'])\n",
    "\n",
    "# now combine using a unary operation (lets do element by element multiplication)\n",
    "\n",
    "df_prod = df0 * df1\n",
    "\n",
    "# note the NaNs in all the places where there is a mismatch between the two original dfs\n",
    "display(df_prod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar to above example, but now make two dfs that are the same size, but only a subset of the labels match\n",
    "* To demonstrate that combining dfs relies on a union of both row (index) and column labels, have one of each mismatch \n",
    "* Write this as if we made a common coding error and mislabeled some of index/column labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first df will be 4 x 4\n",
    "r0 = 4\n",
    "c0 = 4\n",
    "\n",
    "df0 = pd.DataFrame(np.random.rand(r0,c0), index=['r1', 'r2', 'rr3','r4'], columns=['a', 'b', 'c', 'd'])\n",
    "\n",
    "# second df will be 4 x 5\n",
    "r1 = 4\n",
    "c1 = 4\n",
    "\n",
    "df1 = pd.DataFrame(np.random.rand(r1,c1), index=['r1', 'r2', 'r3','r4'], columns=['a', 'b', 'cv', 'd'])\n",
    "\n",
    "# now combine using a unary operation (lets do element by element multiplication)\n",
    "\n",
    "df_prod = df0 * df1\n",
    "\n",
    "# note the NaNs in all the places where there is a mismatch \n",
    "# between the index/column labels in two original dfs\n",
    "display(df_prod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just as with the Series objects, we can use most NumPy operations on DFs without any trouble "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true matrix mulitplication (not element by element)\n",
    "np.dot(df0, df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a high-level summary of the data using built-in functionality of DataFrame object\n",
    "[API reference page](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first call this using the defaults\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can specify different values to change behavior...\n",
    "df.describe(percentiles=np.linspace(0,1,11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And also just like the Series object, you can also apply many other standard analyses using the DF object directly\n",
    "\n",
    "* [Pandas doc for all functions](https://pandas.pydata.org/pandas-docs/stable/api.html#api-dataframe-stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given our main DF with the data from the simulated unit recording study, demonstrate some simple file I/O\n",
    "* Start by saving the data in a csv (comma separated values) file, which is a common format that is useful for small-to-medium sized data and can easily store mixed data types (strs, ints, floats, etc)\n",
    "\n",
    "[more than you ever wanted to know about the CSV format](https://www.loc.gov/preservation/digital/formats/fdd/fdd000323.shtml)\n",
    "\n",
    "[API reference page for csv read/write in Pandas](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out API reference page for all the flags, but for the moment\n",
    "# the we'll save our header in the file so that reading it back in is easier...\n",
    "# can do this either way, you just need to know how a file was written when \n",
    "# go to read it \n",
    "df.to_csv('spike_rates.csv',index=True,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have a look at the csv file in a text editor..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use our current working directory to build a path to the file\n",
    "print(cwd)\n",
    "file_name = cwd + '/spike_rates.csv'\n",
    "print(file_name)\n",
    "\n",
    "# read back in the .csv file that we made above. \n",
    "# by default read_csv will try to infer the column headers but since\n",
    "# we explicitly wrote them out (header=True on write) we can tell \n",
    "# read_csv that row 0 in the file has the header info...\n",
    "# same for the row labels (index labels)...tell\n",
    "# read_csv that our row labels are in the first column (0) \n",
    "# of the file\n",
    "df = pd.read_csv(file_name, index_col=0, header=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making cooler DataFrame styles (and more useful...although that should take a backseat to coolness)\n",
    "[Check here for a bunch of neat style options](https://pandas.pydata.org/pandas-docs/stable/style.html)\n",
    "* Simple demo - can write custom functions that highlight specific aspects of your data - can be very useful for more clearly highlighting/communicating key points in the data within a notebook  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# highlight the max value in each column in yellow...\n",
    "def highlight_max_value_in_columns(data_frame):\n",
    "    ind_max = data_frame == data_frame.max()\n",
    "    return ['background-color: yellow' if i else '' for i in ind_max]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply it here!\n",
    "df.style.apply(highlight_max_value_in_columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
