{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 030: Permutation (randomization) testing and bootstrapping...non-parametraic statistics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal of tutorial: compare standard stats and non-parametric approaches and develop an intuition about why, in many cases, non-parametric stats are preferred\n",
    "* In parametric NHT stats, we make a lot of assumptions about our data and how it is sampled and distributed (e.g. normal distribution, independent samples) and that we can then use standardized distributions (e.g. t-distribution) to make inferences about the population.\n",
    "\n",
    "* Using non-parametric randomization and bootstrapping approaches, we we can back away from many of the assuptions usually associated with our stats. \n",
    "\n",
    "* We can collect our data and then directly evaluate our confidence in the summary statistics (and also differences between conditions) without resorting to comparing to standardized distributions.\n",
    "\n",
    "* Sometimes, i.e. when you have lots of data and all assumptions are met, these approaches will converge on the answers you get from parametric stats.\n",
    "\n",
    "* However, there are many cases where the methods will diverge, and as seen in the tutorial below, there are good reasons to favor these non-parametric approaches over standard statistical test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomization tests\n",
    "In a typical study, you collect data from two groups (or more) and then\n",
    "you want to decide if the two group means are different given the separation \n",
    "of their means and their variances. So you compute the differences\n",
    "between the means, weight that difference by the variance, and then\n",
    "compare that test statistic again a standard look up table (e.g. a\n",
    "t-distribution or similar) to get a p-value.  \n",
    "This standard approach makes a number of assumptions:\n",
    "* The data are normally distributed so that the test stat follows the expected distribution (i.e. a t-distribution)\n",
    "* That we have taken random and independent samples from the population(s) \n",
    "* We use a standard look-up-table to put a p-value on our test statistic so that we can genearlize our results to the entire population\n",
    "\n",
    "In contrast, *RANDOMIZATION TESTING* relaxes many of these assumptions in place of a different approach.\n",
    "* We do not need normally distributed data, etc. In fact, we rarely seem to meet this condition in real data anyway, so this is a pretty handy thing. We also relax the independent and random sampling assumptions - we are just going to deal the data that we have in hand and take it as it comes. \n",
    "* With randomization testing, we are not able to make a population inference (see bootstrapping section below for notes about that). \n",
    "* Instead, we are evaluating how likely our summary statistic is given the null hypothesis that our experimental manipulations have no effect (or phrased a different and more useful way - the null hypthesis that our experimental condition labels have no effect on the outcome of the summary statistic). \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "The test statistic that we generate is not compared to a standard look up table - instead we generate our own 'null' distribution against which to compare the data that we collect to evalute the propbability of obtaining a test statistic of the observed magnitude given that our condition labels (or our manipulation) had no effect.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import our standard numpy and plotting stuff...also stats from scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats  # has t-tests and other stats stuff...\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# also define the default font we'll use for figures. \n",
    "fig_font = {'fontname':'Arial', 'size':'20'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start by generating some fake data from two experimental conditions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100                      # number of data points (or 'subjects')\n",
    "mu_of_conditions = 1.4,1     # means of each distribution\n",
    "var_of_conditions = 1,1      # variance of each distribution - start with equal variances\n",
    "\n",
    "# then use random.randn to generate two data sets with specified \n",
    "d1 = (np.random.randn(N,) * var_of_conditions[0]) + mu_of_conditions[0]\n",
    "d2 = (np.random.randn(N,) * var_of_conditions[1]) + mu_of_conditions[1]\n",
    "\n",
    "# histogram to show the data. Use alpha value to set transparency so that you can see overlap \n",
    "plt.hist(d1, color='r', alpha=.5)\n",
    "plt.hist(d2, color='g', alpha=.5)\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# do a t-tests using scipy.stats (note - this is repeated measures t-test, or 'related samples')\n",
    "t_val = stats.ttest_rel(d1,d2)\n",
    "\n",
    "# print out t and p-values\n",
    "print('t-value: ', np.round(t_val.statistic,2), ' p-value: ', np.round(t_val.pvalue,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now generate a set of t-values across repeated iterations of this 'study'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N = 100                     # number of data points (or 'subjects')\n",
    "mu_of_conditions = 1.4,1    # means of each distribution\n",
    "var_of_conditions = 1,1     # variance of each distribution - start with equal variance, then make unequal\n",
    "\n",
    "# number of repeats of the study\n",
    "num_studies = 1000\n",
    "\n",
    "# allocate array for storing t-values\n",
    "study_t_val = np.zeros(num_studies)\n",
    "\n",
    "for i in np.arange(num_studies):\n",
    "    # then use random.randn to generate two data sets with specified \n",
    "    d1 = (np.random.randn(N,) * var_of_conditions[0]) + mu_of_conditions[0]\n",
    "    d2 = (np.random.randn(N,) * var_of_conditions[1]) + mu_of_conditions[1]\n",
    "\n",
    "    t_val = stats.ttest_rel(d1,d2)\n",
    "    \n",
    "    # store the t-value each time...\n",
    "    study_t_val[i] = t_val.statistic\n",
    "    \n",
    "# histogram to show the data. \n",
    "# can specify the number of bins to use in the call to plt.hist\n",
    "h = plt.hist(study_t_val, color='r', alpha=1, bins=30)\n",
    "plt.xticks(**fig_font)\n",
    "plt.yticks(**fig_font)\n",
    "plt.xlabel('T-value', **fig_font)\n",
    "plt.ylabel('Count', **fig_font)\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now do this the faster, numpy way...ditch the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100                     # number of data points (or 'subjects')\n",
    "mu_of_conditions = 1.1,1      # means of each distribution\n",
    "var_of_conditions = 1,1     # variance of each distribution - start with equal variance, then make unequal\n",
    "\n",
    "# number of times we repeat the study\n",
    "num_studies = 1000\n",
    "\n",
    "# then use random.randn to generate two data sets with specified \n",
    "# instead of looping generate a N x num_studies matrix all at once \n",
    "d1 = (np.random.randn(N,num_studies) * var_of_conditions[0]) + mu_of_conditions[0]\n",
    "d2 = (np.random.randn(N,num_studies) * var_of_conditions[1]) + mu_of_conditions[1]\n",
    "\n",
    "# do the t-test on all columns of data (i.e. across rows, or across the first dim)\n",
    "t_val = stats.ttest_rel(d1,d2,axis=0)\n",
    "    \n",
    "# store the t-value each time...\n",
    "study_t_val = t_val.statistic       \n",
    "    \n",
    "# histogram to show the data. Use alpha value to set transparency so that you can see overlap \n",
    "plt.hist(study_t_val, color='r', alpha=1, bins=30)\n",
    "plt.xticks(**fig_font)\n",
    "plt.yticks(**fig_font)\n",
    "plt.xlabel('T-value', **fig_font)\n",
    "plt.ylabel('Count', **fig_font)\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now set things up to try a randomization test\n",
    "* Start: we'll generate only one set of experimental data\n",
    "* Then we'll compute and store the t-value that we observe (i.e. we just do our normal analysis)\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "Run this cell of code a few times with 1.3 and 1 as the means and equal variance 1,1 for both conditions. You should see that you often get a null result. Run it a few more times...sometimes the p-value is >4.5! \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100                      # number of data points (or 'subjects')\n",
    "mu_of_conditions = 1.3,1       # means of each distribution\n",
    "var_of_conditions = 1,1      # variance of each distribution - start with equal variance, then make unequal\n",
    "\n",
    "# number of times we repeat the the study - set to 1 for this demo of the randomization test\n",
    "# as if you just collected your data set and then you're sitting down to do the stats. \n",
    "num_studies = 1\n",
    "\n",
    "# make our data sets\n",
    "d1 = (np.random.randn(N,num_studies) * var_of_conditions[0]) + mu_of_conditions[0]\n",
    "d2 = (np.random.randn(N,num_studies) * var_of_conditions[1]) + mu_of_conditions[1]\n",
    "\n",
    "# do the t-test on all columns of data\n",
    "# will write out the formula for the t-test here because it will make the randomization code \n",
    "# easier to understand: mean difference divided by the S.E. of the difference\n",
    "study_t_val = np.mean(d1-d2, axis=0) / (np.std(d1-d2, axis=0) / np.sqrt(N-1))\n",
    "\n",
    "# compute our p-value using stats.t.cdf (could also get this from stats.ttest_x but lets compute by hand)\n",
    "p_value = 2*(1-stats.t.cdf(study_t_val, N-1))\n",
    "\n",
    "# print out our test statistic and our p-value\n",
    "print('Tval: ', study_t_val, 'Pval: ', p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next step: generate the actual distribution of your test statistic under the null that your experimental conditions don't matter\n",
    "* Goal is to test the likelihood of observing our actual t-value under the null hypothesis that the experimental manipulation has no effect\n",
    "* Another way of stating the null is that \"condition labels shouldn't matter\" (because if the condition has no effect, then shouldn't matter whether the data point came from condition A or B). \n",
    "* So we can randomly assign the data points as coming from either condition A or B, with the constraint that we keep the balance of data points in condition A and B the same as in the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many times do we want to randomize the data and then eval our\n",
    "# test statistic again?\n",
    "num_randomizations = 1000\n",
    "\n",
    "# allocate a matrix to store the output\n",
    "rand_t = np.zeros(num_randomizations)\n",
    "\n",
    "# matrix to store our t-values under the null that condition labels don't matter\n",
    "null_t_val = np.zeros(num_randomizations)\n",
    "\n",
    "# start a loop to iterate over randomizations of condition labels\n",
    "for i in np.arange(num_randomizations):\n",
    "    \n",
    "    # conceptualize the randomization as randomly changing the sign of the difference \n",
    "    # between each pair of entries in d1 and d2\n",
    "    # note: bonus points for figuring out one small potential flaw in this line of code...\n",
    "    rand_data = np.sign(np.random.rand(N,num_studies)-.5) * (d1-d2)\n",
    "    \n",
    "    null_t_val[i] = np.mean(rand_data, axis=0) / (np.std(rand_data, axis=0) / np.sqrt(N-1))\n",
    "    \n",
    "#show the distribution of tvals under the null\n",
    "plt.hist(null_t_val, color='r', alpha=1, bins=30)\n",
    "plt.xlabel('T-value')\n",
    "plt.ylabel('Count')\n",
    "plt.show()    \n",
    "\n",
    "# now print out our mean t-value under the null, and the associated pvalue\n",
    "# that is generated by comparing our observed t-value with the distribution\n",
    "# of tvalues under the null!\n",
    "rand_p_value = 2*(1-(np.sum(study_t_val>null_t_val) / num_randomizations))\n",
    "print('Randomization-based p-value: ', np.round(rand_p_value,4), ' Parametric P-value: ', np.round(p_value,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So the two pvalues are about the same...and that's good becuase all the assumptions of the parametric t-test were met (normally distributed data, etc). But this is not always true when we violate assumptions\n",
    "* lets see what happens here when we violate assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some fake data...\n",
    "d1 = np.array([2,1,2,1,0,1,8,9,11,9,11,14,13])\n",
    "d2 = np.array([2,3,3,2,1,1,7,10,12,10,10,13,12])\n",
    "\n",
    "# figure out how many data points \n",
    "N = len(d1)\n",
    "\n",
    "# figure out correlation between them...\n",
    "corr = np.corrcoef(d1,d2)\n",
    "print(corr, '\\n')\n",
    "\n",
    "# Neat trick...index the function call. \n",
    "corr = np.corrcoef(d1,d2)[0,1]\n",
    "print(corr, '\\n')\n",
    "\n",
    "# compute the t-value/p-value corresponding to the correlation\n",
    "t_val = (corr*np.sqrt(N-2)) / np.sqrt(1-corr**2)\n",
    "p_value = 2*(1-stats.t.cdf(t_val, N-1))\n",
    "print('Tval: ', t_val, 'Pval: ', p_value)\n",
    "\n",
    "# plot the data...\n",
    "plt.scatter(d1,d2,color='r')\n",
    "plt.title('YES...Nature paper!')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out how many times we want to run the randomization test...\n",
    "num_randomizations = 1000\n",
    "rand_corr = np.zeros(num_randomizations)\n",
    "rand_t_val = np.zeros(num_randomizations)\n",
    "tmp0 = np.zeros(N)\n",
    "tmp1 = np.zeros(N)\n",
    "\n",
    "# start a loop over randomization iterations\n",
    "for i in np.arange(num_randomizations):\n",
    "    \n",
    "    # write this out explicitly for clarity - randomly assigning numbers from d1 or d2\n",
    "    # i.e. condition doesn't matter. \n",
    "    for j in np.arange(N):   \n",
    "        if np.random.rand(1) < .5:\n",
    "            tmp0[j] = d1[j]\n",
    "            tmp1[j] = d2[j]\n",
    "        else:\n",
    "            tmp0[j] = d2[j]\n",
    "            tmp1[j] = d1[j]\n",
    "\n",
    "    # then correlate the two randomized data vectors...compute tvalues\n",
    "    rand_corr[i] = np.corrcoef(tmp0,tmp1)[0,1]\n",
    "    rand_t_val[i] = (rand_corr[i]*np.sqrt(N-2)) / np.sqrt(1-rand_corr[i]**2)\n",
    "\n",
    "# compute the p-value of our real t-score (t_val) vs our radomized distribution\n",
    "rand_p_value = 2*(1-(np.sum(t_val>rand_t_val) / num_randomizations))\n",
    "print('Randomization-based p-value: ', np.round(rand_p_value,8), ' Parametric P-value: ', np.round(p_value,8))\n",
    "\n",
    "# plotting the distribution of p-values that we observe under the null\n",
    "plt.hist(rand_t_val, color='r', alpha=1, bins=30)\n",
    "plt.xlabel('T-value under null')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Bummer...have to send to Science')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using bootstrapping to estimate true CIs on your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic background: Imagine you do a typical experiment in your lab. \n",
    "* You run a subject  (human/rodent/whatever) through your memory/perception test and you get a 100 numbers, one number that indexes the magnitude of your  dependent variable on each trial (RTs/Accuracy/EEG amplitude/spike rate/GSR/etc).\n",
    "* Now, you compute your mean over your 100 samples and get 10 (spikes/%cor/etc). \n",
    "* Now you might reasonably wonder: this subject produced a mean score of  10 in my experiment, but what if I were to run her again? Would I get the same number? What if she participated 1,000 times in the experiment? Would I get 10 every time? \n",
    "* Could ask the same question about generalizing to a population based on sample mean, variance, etc\n",
    "* So the problem is that you're in a situation where you want to know the reliability of a number (i.e. the central tendency of 10), but you only get to make 1 set of measurements. \n",
    "* There are several ways that you can estimate the number's reliability, but one of the best is to use bootstrapping to estimate confidence intervals on your summary statistics (mean, variance, t-val, etc) by resampling the observed data with replacement\n",
    "* The idea is that you can estimate how certain you are of a summary stat, and you can do that either by taking more and more and more observations of data (often prohibitive) or by sampling a reasonable amount of data and then bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First lets look at putting confidence intervals on a sample mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a huge data set that is meant to represent our entire population \n",
    "# (i.e. we sample from all people on earth)\n",
    "N = 100000\n",
    "pop_mean = 100\n",
    "pop_var = 10\n",
    "pop_data = (np.random.randn(N) * pop_var) + pop_mean\n",
    "\n",
    "# now lets run an experiment and just sample a few people\n",
    "S_N = 10\n",
    "\n",
    "# get a random set of S_N values from the pop data (could do all in one line...)\n",
    "# note that this is sampling without replacement\n",
    "index = np.random.permutation(N)[0:S_N]\n",
    "\n",
    "# use that index to grab the data...\n",
    "samp_data = pop_data[index]\n",
    "\n",
    "# print the mean of our sample data\n",
    "print(np.mean(samp_data,axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a bunch of experiments to get a feel for how variable your estimate of the mean is based on sample size (this is basic central limit theorum stuff). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many subjects per experiment\n",
    "S_N = 2 # 10,20,1000,N\n",
    "\n",
    "# now do this a whole bunch of times and see how much our sample mean varies wrt the true mean\n",
    "num_experiments = 500\n",
    "samp_mean = np.zeros(num_experiments)\n",
    "\n",
    "for i in np.arange(num_experiments):\n",
    "    # get a random set of S_N values from the pop data (could do all in one line...)\n",
    "    index = np.random.permutation(N)[0:S_N]\n",
    "\n",
    "    # use that index to grab the data...\n",
    "    samp_data = pop_data[index]\n",
    "    \n",
    "    # compute mean of our samples from each experiment that we did\n",
    "    samp_mean[i] = np.mean(samp_data,axis=0)\n",
    "\n",
    "# histogram it\n",
    "plt.hist(samp_mean, color='r', alpha=1, bins=30)\n",
    "plt.axvline(pop_mean, color='k', linewidth=1)\n",
    "plt.xlabel('Sample mean')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So as the sample size goes up, we better approximate the population distribution...no surprise there. \n",
    "\n",
    "* However, in practice, you only sample some finite set of data in a given experiment\n",
    "* Based on that sample, you can get an estimate of the mean (or other summary statistic)\n",
    "* But how good is that estimate? How accurately does it reflect the true population parameter? \n",
    "* To really know, you could go out and sample the entire population\n",
    "* Or you can take your sample data and repeatedly estimate the statistic of interest after drawing a new set of data using resampling with replacement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many subjects per experiment...start with 20\n",
    "S_N = 20\n",
    "\n",
    "# grab a set of S_N samples from the population data\n",
    "samp_data = pop_data[np.random.permutation(N)[0:S_N]]\n",
    "\n",
    "# now do this a whole bunch of times and see how much our sample mean varies wrt the true mean\n",
    "num_bootstraps = 5000\n",
    "boot_mean = np.zeros(num_bootstraps)\n",
    "\n",
    "# now loop over bootstrap iterations. Each time generate a \"new\" data set\n",
    "# by resampling our observed data with replacement\n",
    "for i in np.arange(num_bootstraps):\n",
    "    # get a random set of values from 0:S_N (exclusive) with replacement \n",
    "    # so same entry can happen more than once...\n",
    "    boot_data = samp_data[np.random.randint(S_N, size=S_N)]\n",
    "\n",
    "    # compute mean of our samples from each experiment that we did\n",
    "    boot_mean[i] = np.mean(boot_data,axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now compute the mean and CIs of our bootstrapped distribution\n",
    "* Take the lower and upper 2.5% of our distribution...\n",
    "* Change sample size...\n",
    "* This lets you say something about the population values even though you didn't actually collect any more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first compute the mean of the bootstrapped data. \n",
    "mean_of_boot_data = np.mean(boot_mean, axis=0)\n",
    "\n",
    "# 95% CIs based on percentiles \n",
    "CIs = np.percentile(boot_mean, [2.5, 97.5])\n",
    "\n",
    "# histogram it\n",
    "plt.hist(boot_mean, color='r', alpha=1, bins=30)\n",
    "plt.axvline(pop_mean, color='k', linewidth=1)\n",
    "plt.axvline(CIs[0], color='k', linewidth=1)\n",
    "plt.axvline(CIs[1], color='k', linewidth=1)\n",
    "plt.xlabel('Sample mean')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So even though our sample was a little off the true population mean, we can place CIs on our data to figure out how confident that we are in the stability of our sample!\n",
    "* To develop an intuition about why, and to see how this can be used for statistical inference, lets go back to our correlation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some fake data...\n",
    "N = 16\n",
    "d1 = np.random.randn(N)\n",
    "d2 = np.random.randn(N)\n",
    "\n",
    "# put in an outlier at the end\n",
    "d1[-1] = 20\n",
    "d2[-1] = 15\n",
    "\n",
    "# plot the data...pretty sweet correlation!\n",
    "plt.scatter(d1,d2,color='r')\n",
    "plt.show()\n",
    "\n",
    "# correlation...\n",
    "obs_corr = np.corrcoef(d1,d2)[0,1]\n",
    "print('pretty sweet correlation! ', obs_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can use bootstrapping to estimate CIs on our correlation value, and can see if those CIs include 0\n",
    "* generate 'new' data sets by resampling with replacement. Do this across pairs of corresponding points to preserve the pairings...however, not all pairs will be in each iteration and some pairs will be in there multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# number of bootstraps\n",
    "num_bootstraps = 1000\n",
    "corr = np.zeros(num_bootstraps)\n",
    "\n",
    "for i in np.arange(num_bootstraps):\n",
    "    # with replacement generate a sample number from 0:N exclusive and do that N times\n",
    "    index = np.random.randint(N, size=N)\n",
    "\n",
    "    # use that to pull data from each of our arrays\n",
    "    tmp1 = d1[index]\n",
    "    tmp2 = d2[index]\n",
    "    \n",
    "    # compute correlation\n",
    "    corr[i] = np.corrcoef(tmp1,tmp2)[0,1]\n",
    "    \n",
    "# then compute 95% CIs based on percentiles \n",
    "CIs = np.percentile(corr, [2.5, 97.5])\n",
    "\n",
    "# histogram it\n",
    "plt.title('Maybe its not a Nature paper after all...')\n",
    "plt.hist(corr, color='r', alpha=1, bins=30)\n",
    "plt.axvline(obs_corr, color='k', linewidth=1)\n",
    "plt.axvline(CIs[0], color='k', linewidth=1)\n",
    "plt.axvline(CIs[1], color='k', linewidth=1)\n",
    "plt.xlabel('Correlation distribution')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happened here?\n",
    "* On most of the bootstrap data sets, the outlier isn't in there. In that case, the data are not really assoicated, and so the correlation values cluster around zero (only small data set, so maybe not exactly 0)\n",
    "* However, there are many high correlations whenever that outlier happens to be included in the resampled data\n",
    "* Take home - this analysis, unlike parametric approaches, gives you a much more realistic view about how accurately your summary statistics actually reflect the real population level values!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short demo on how you might compare data from two conditions using this approach...\n",
    "* Generate two correlations, see if they are different\n",
    "* Compute CIs for each, and if the CIs don't overlap, you are good to go...no more analysis needed!\n",
    "* Note that I'm using a simple hack here to generate some correlated values...if you want to do this right see the link below\n",
    "\n",
    "[scipy cookbook on generating correlated values](https://scipy-cookbook.readthedocs.io/items/CorrelatedRandomSamples.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some fake data...then try N=160 and noise = 20 etc...\n",
    "N = 16\n",
    "noise = 10\n",
    "\n",
    "# first pair of data vectors from one experimental condition (say - off drug)\n",
    "d1 = noise * np.random.rand(N)+np.linspace(.1,20,N)\n",
    "d2 = noise * np.random.rand(N)+np.linspace(.1,20,N)\n",
    "\n",
    "# first pair of data vectors from second experimental condition (say - on drug)\n",
    "d3 = noise * np.random.rand(N) + np.linspace(.1,30,N)\n",
    "d4 = noise * np.random.rand(N) + np.linspace(.1,2,N)\n",
    "\n",
    "# show the data set...\n",
    "plt.scatter(d1,d2, color='r', linewidths=3)\n",
    "plt.scatter(d3,d4, color='g', linewidths=3)\n",
    "plt.xlabel('Variable 1')\n",
    "plt.ylabel('Variable 2')\n",
    "plt.legend(['Off drug', 'On drug'])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now take the data, resample with replacement, and estimate CIs for each condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of bootstraps\n",
    "num_bootstraps = 1000\n",
    "corr1 = np.zeros(num_bootstraps)\n",
    "corr2 = np.zeros(num_bootstraps)\n",
    "\n",
    "# bootstrapping loop...\n",
    "for i in np.arange(num_bootstraps):\n",
    "    # with replacement generate a sample number from 0:N exclusive and do that N times\n",
    "    index = np.random.randint(N, size=N)\n",
    "\n",
    "    # use that to pull data from each of our arrays\n",
    "    tmp1 = d1[index]\n",
    "    tmp2 = d2[index]\n",
    "    tmp3 = d3[index]\n",
    "    tmp4 = d4[index]    \n",
    "    \n",
    "    # compute correlation between first two vectors. \n",
    "    corr1[i] = np.corrcoef(tmp1,tmp2)[0,1]\n",
    "    \n",
    "    # then between the next two vectors.\n",
    "    corr2[i] = np.corrcoef(tmp3,tmp4)[0,1]\n",
    "\n",
    "    \n",
    "# then compute 95% CIs based on percentiles \n",
    "CIs_1 = np.percentile(corr1, [2.5, 97.5])\n",
    "CIs_2 = np.percentile(corr2, [2.5, 97.5])\n",
    "\n",
    "# plot\n",
    "plt.title('Correlation values +- 95% CIs')\n",
    "plt.hist(corr1, color='r', alpha=.25, bins=30)\n",
    "plt.axvline(CIs_1[0], color='r', linewidth=1)\n",
    "plt.axvline(CIs_1[1], color='r', linewidth=1)\n",
    "plt.hist(corr2, color='g', alpha=.25, bins=30)\n",
    "plt.axvline(CIs_2[0], color='g', linewidth=1)\n",
    "plt.axvline(CIs_2[1], color='g', linewidth=1)\n",
    "plt.xlabel('Bootstrapped correlations')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now go back and regenerate the fake data with more samples...much more confident in our results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take home\n",
    "* Often, our data violate assumptions of typical stats (non-normal, non-independent, etc). \n",
    "* Randomization (permutation) testing gives you a way to eval your summary statistics while accounting for these flaws\n",
    "* However, the p-value you get from randomization testing is specific to your data set (and all of its flaws)! Keep that in mind when making inferences\n",
    "* Bootstrapping provides an less-assumption-laden way of estimating CIs based on resampling the data with replacement.\n",
    "* The CIs from your data set will be good estimates of the true values so long as you have a large enough initial sample to reasonably do the resampling with replacement (you can generate enough unique permutations)\n",
    "* Can also use bootstrapping for statistics - non-overlapping CIs are a pretty compelling, and generally conservative, way to estimate differences between data from two conditions \n",
    "* Note: there are some conditions when bootstrapping might not be ideal, like if you have long-tailed distributions. In that case, your sample data set may not capture that aspect of the data and extreme values will be missed. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
