{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial04 Intro to machine learning/pattern recognition, Part II\n",
    "* Here, instead of fitting a data set and then deciding which 'cloud' of training data is closer to each test vector, we'll just focus on drawing a boundary that maximizes the margin between points that are close to the dividing line (plane/hyperplane) that separates the two data sets.\n",
    "* This is the approach implemented by Support Vector Machines (SVMs)\n",
    "* Often effecient because you don't need to model the full data set - just need to maximize the margin\n",
    "* Also does not rely on accurate estimation of the covariance matrix, which is problematic with small data sets (especially if you have fewer samples or trials than variables). \n",
    "\n",
    "[inspiration for this tutorial provided by this book](http://shop.oreilly.com/product/0636920034919.do)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules...add some sklearn functionality for the SVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import eigh\n",
    "\n",
    "# Support vector classifier\n",
    "from sklearn.svm import SVC \n",
    "\n",
    "# also define the default font we'll use for figures. \n",
    "fig_font = {'fontname':'Arial', 'size':'20'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data generating function from the last tutorial..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_data_pnts = number of experimental trials\n",
    "# mu0, mu1 = 2 element np.array that sets the mean of each variable in each condition\n",
    "# cov0, cov1 = symetric 2 x 2 matrix with main diag specifying the variance of each variable \n",
    "# in each condition, and the off diag elements specifying the covariance\n",
    "\n",
    "def gen_cor_data(num_data_pnts, mu0, mu1, cov0, cov1, plot):\n",
    "\n",
    "    # number of variables, in this case lets keep it at 2 because that makes it easy to \n",
    "    # visualize\n",
    "    V = 2\n",
    "\n",
    "    # means of each variable in each condition\n",
    "    mean_of_data0 = mu0; \n",
    "    mean_of_data1 = mu1 \n",
    "\n",
    "    # generate some random data vectors drawn from normal\n",
    "    data0 = np.random.randn(num_data_pnts,V) \n",
    "    data1 = np.random.randn(num_data_pnts,V) \n",
    "\n",
    "    # compute the eigenvalues and eigenvectors of the cov matrix\n",
    "    evals0, evecs0 = eigh(cov0)\n",
    "    evals1, evecs1 = eigh(cov1)\n",
    "\n",
    "    # Construct c, so c*c^T = cov.\n",
    "    c0 = np.dot(evecs0, np.diag(np.sqrt(evals0)))\n",
    "    c1 = np.dot(evecs1, np.diag(np.sqrt(evals1)))\n",
    "\n",
    "    # convert the data using by multiplying data by c\n",
    "    # to be consistent with previous tutorials, we want the data running down columns...so do the double .T\n",
    "    cdata0 = np.dot(c0, data0.T).T\n",
    "\n",
    "    # then add in the mu offset...use np.ones * each condition mean\n",
    "    cdata0 += np.hstack((np.ones((num_data_pnts,1))*mean_of_data0[0], np.ones((num_data_pnts,1))*mean_of_data0[1]))\n",
    "\n",
    "    # repeat for the data from the second experimental condition \n",
    "    cdata1 = np.dot(c1, data1.T).T\n",
    "    cdata1 += np.hstack((np.ones((num_data_pnts,1))*mean_of_data1[0], np.ones((num_data_pnts,1))*mean_of_data1[1])) \n",
    "\n",
    "    if plot:\n",
    "        # plot the data...\n",
    "        plt.scatter(cdata0[:,0], cdata0[:,1], color='b')\n",
    "        plt.scatter(cdata1[:,0], cdata1[:,1], color='r')\n",
    "        plt.xlabel('Resp Neuron 1', **fig_font)\n",
    "        plt.ylabel('Resp Neuron 2', **fig_font)\n",
    "        plt.legend(['Condition 1', 'Condition 2'])\n",
    "        plt.show()\n",
    "    \n",
    "    return cdata0, cdata1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data using our trusty function\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# number of trials\n",
    "N = 100\n",
    "\n",
    "# means\n",
    "mu0 = np.array([1,5])\n",
    "mu1 = np.array([5,1])\n",
    "\n",
    "# variance of each neuron\n",
    "var_of_neuron0 = 3.2\n",
    "var_of_neuron1 = 3.2\n",
    "\n",
    "# covariance of neurons in each experimental condition \n",
    "cov_cond_0 = 2.5\n",
    "cov_cond_1 = 2.5\n",
    "\n",
    "cov0 = np.array([\n",
    "        [var_of_neuron0, cov_cond_0],\n",
    "        [cov_cond_0, var_of_neuron1]\n",
    "    ])\n",
    "\n",
    "cov1 = np.array([\n",
    "        [var_of_neuron0, cov_cond_1],\n",
    "        [cov_cond_1, var_of_neuron1]\n",
    "    ])\n",
    "\n",
    "# generate the data and plot...\n",
    "d0, d1 = gen_cor_data(num_data_pnts=N, mu0=mu0, mu1=mu1, cov0=cov0, cov1=cov1, plot=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now think about drawing a decision boundary that will maximally separate these two classes (i.e. a boundary that will separate data from these two experimental conditions)\n",
    "* In the case of 2D data like this, then we're looking to just draw a line that separates the two classes\n",
    "* In higher dim data sets, then we're trying to find the plane (3D) or hyperplane (ND). \n",
    "* However, the logic is the same in all cases\n",
    "* **Important**: Note that there are many lines that can perfectly separate these two classes!\n",
    "* Given this, how do we select the optimal boundary? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some lines that separate these two classes\n",
    "\n",
    "# start with an x-axis that covers our space\n",
    "x_range = -4,10\n",
    "x_vals = np.linspace(x_range[0], x_range[1])\n",
    "\n",
    "# plot data - note the abbreviated 'color' call\n",
    "plt.scatter(d0[:, 0], d0[:, 1], c='b')\n",
    "plt.scatter(d1[:, 0], d1[:, 1], c='r')\n",
    "plt.xlabel('Resp Neuron 1', **fig_font)\n",
    "plt.ylabel('Resp Neuron 2', **fig_font)\n",
    "\n",
    "# draw some lines...y = ax+b...\n",
    "for a, b in [(.75, 0.65), (.7, 0.15), (1.15, -.1)]:\n",
    "    plt.plot(x_vals, a * x_vals + b, '-k')\n",
    "\n",
    "plt.xlim(x_range[0], x_range[1])\n",
    "\n",
    "# add an extra data point to make a point about \n",
    "plt.title('Which class does this green dot belong to?')\n",
    "plt.plot(6, 6, 'o', c='g', markersize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given the ambiguity here about where to draw the boundary, SVMs will instead create a line (or plane or hyperplane) with a margin that extends up to the nearest points in each data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an x-axis that covers our space\n",
    "x_range = -4,10\n",
    "x_vals = np.linspace(x_range[0], x_range[1])\n",
    "\n",
    "# plot data - note the abbreviated 'color' call\n",
    "plt.scatter(d0[:, 0], d0[:, 1], c='b')\n",
    "plt.scatter(d1[:, 0], d1[:, 1], c='r')\n",
    "plt.xlabel('Resp Neuron 1', **fig_font)\n",
    "plt.ylabel('Resp Neuron 2', **fig_font)\n",
    "\n",
    "# plot a sample line + margin that maximizes distance between boundary points.\n",
    "# note the specification of a 3 element vector to loop over...cool feature.\n",
    "for a, b, c in [(1, -.2, .9)]:\n",
    "    y_vals = a * x_vals + b\n",
    "    plt.plot(x_vals, y_vals, '-k')\n",
    "    plt.fill_between(x_vals, y_vals + c, y_vals - c,\n",
    "                     color='g', alpha=0.2)\n",
    "\n",
    "plt.xlim(x_range[0], x_range[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So above was just a hand-drawn version of what an SVM is trying to accomplish. Now we can actually implement to see how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first lets assign parts of our data to train and test sets\n",
    "# this is the same approach that we used for the distance based classifiers in the \n",
    "# first part of the tutorial.\n",
    "percent_train_set = .9\n",
    "r,c = d0.shape  # get the shape of our data sets\n",
    "trn_length = int(np.round(percent_train_set*r, 0))\n",
    "\n",
    "# now assign the first 90% of the trials to the training set...\n",
    "# we'll stack up the first 90% from the first condition on top \n",
    "# of the 90% of data from the second condition\n",
    "trn_data = np.vstack((d0[0:trn_length,:], d1[0:trn_length,:]))\n",
    "\n",
    "# and the last 10% of the trials get stacked to make the test set...\n",
    "tst_data = np.vstack((d0[trn_length:,:], d1[trn_length:,:]))\n",
    "\n",
    "# last generate a vector that labels the data from each trial\n",
    "# as belonging to condition 1 or condition 2\n",
    "trn_labels = np.hstack((np.zeros(trn_length), np.ones(trn_length))).T\n",
    "tst_labels = np.hstack((np.zeros(r-trn_length), np.ones(r-trn_length))).T\n",
    "\n",
    "# The SVC was imported earlier via the sklearn module\n",
    "# so we can train the classifer using our data.\n",
    "\n",
    "# first specify the model...linear SVM with a large criterion value\n",
    "# if C is big, that will enforce a hard margin...if C is small, it \n",
    "# will allow some leakage of points over the margin.\n",
    "model = SVC(kernel='linear', C=1E10)\n",
    "\n",
    "# then fit the model to our training data by passing in the data matrix + a list of labels that denotes the \n",
    "# experimental condition for each trial\n",
    "model.fit(trn_data, trn_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now take a look at the model and how it separates the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(d0[:, 0], d0[:, 1], color='b')\n",
    "plt.scatter(d1[:, 0], d1[:, 1], color='r')\n",
    "\n",
    "# grab a ui (handle) for the axis\n",
    "# then get the x limits and the y limits (ranges)\n",
    "axis = plt.gca()\n",
    "x_limits = axis.get_xlim()\n",
    "y_limits = axis.get_ylim()\n",
    "\n",
    "# create grid to evaluate model\n",
    "x = np.linspace(x_limits[0], x_limits[1], 100)\n",
    "y = np.linspace(y_limits[0], y_limits[1], 100)\n",
    "Y, X = np.meshgrid(y, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then eval the model over the span of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack up the data into a 10000,2 matrix\n",
    "XY = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "\n",
    "# compute the decision function across all points \n",
    "# that span the space that the model is fitting\n",
    "# take the decision function and reshape to the size of data\n",
    "DF = model.decision_function(XY).reshape(X.shape)\n",
    "\n",
    "# plot the data\n",
    "plt.scatter(d0[:, 0], d0[:, 1], color='b')\n",
    "plt.scatter(d1[:, 0], d1[:, 1], color='r')\n",
    "\n",
    "# grab the handle for the axis\n",
    "axis = plt.gca()\n",
    "\n",
    "# plot the decision boundary and the margins on either side of boundary\n",
    "axis.contour(X, Y, DF, colors='k',\n",
    "           levels=[-.8, 0, .8],\n",
    "           linestyles=['--', '-', '--'])\n",
    "\n",
    "# plot support vectors as points in the figure\n",
    "axis.scatter(model.support_vectors_[:, 0],\n",
    "           model.support_vectors_[:, 1],\n",
    "           s=100,c='g');\n",
    "\n",
    "axis.set_xlim(x_limits)\n",
    "axis.set_ylim(y_limits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The highlighted points are the 'support vectors' and are the points that define the fit of the boundary that best separates the data associated with each experimental condition. \n",
    "* You can think of these points as 'pivots' or leverage points that constrain the fit of the boundary\n",
    "* Notice also that only the position of the support vectors matters here. The positions of the other points, so long as they are one side or the other of the margin, do not matter. \n",
    "* This stands in contrast the Maha distance classifer that modeled the entire distribution of data in each class...the SVM only cares about maximizing the margin along the boundary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See it in action..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make data sets...then classify with an SVM\n",
    "# for fun, make the same data set that we used with the Maha distance classifier\n",
    "np.random.seed(2)\n",
    "mu0 = np.array([1,4])\n",
    "mu1 = np.array([4,1.5])\n",
    "\n",
    "# variance of each neuron\n",
    "var_of_neuron0 = 3\n",
    "var_of_neuron1 = 3\n",
    "\n",
    "# covariance of neurons in each experimental condition \n",
    "cov_cond_0 = 2\n",
    "cov_cond_1 = 2\n",
    "\n",
    "cov0 = np.array([\n",
    "        [var_of_neuron0, cov_cond_0],\n",
    "        [cov_cond_0, var_of_neuron1]\n",
    "    ])\n",
    "\n",
    "cov1 = np.array([\n",
    "        [var_of_neuron0, cov_cond_1],\n",
    "        [cov_cond_1, var_of_neuron1]\n",
    "    ])\n",
    "\n",
    "# generate the data and plot...\n",
    "d0, d1 = gen_cor_data(num_data_pnts=N, mu0=mu0, mu1=mu1, cov0=cov0, cov1=cov1, plot=1)\n",
    "\n",
    "# now lets generate a training set and a test set. We'll use 90% of our data as a training set\n",
    "# and leave 10% as a test set\n",
    "r,c = d0.shape  # get the shape of our data set (and d1 is the same size, so just need to get one of these)\n",
    "trn_length = int(np.round(.9*r, 0))\n",
    "\n",
    "# now assign the first 90% of the trials to the training set...\n",
    "# we'll stack up the first 90% from the first condition on top \n",
    "# of the 90% of data from the second condition\n",
    "trn_data = np.vstack((d0[0:trn_length,:], d1[0:trn_length,:]))\n",
    "\n",
    "# and the last 10% of the trials to the test set...\n",
    "tst_data = np.vstack((d0[trn_length:,:], d1[trn_length:,:]))\n",
    "\n",
    "# last its helpful to generate a vector that labels the data from each trial\n",
    "# as belonging to condition 1 or condition 2\n",
    "trn_labels = np.hstack((np.zeros(trn_length), np.ones(trn_length))).T\n",
    "tst_labels = np.hstack((np.zeros(r-trn_length), np.ones(r-trn_length))).T\n",
    "\n",
    "# then train the model - Play with C a bit here...\n",
    "model = SVC(kernel='linear', C=1)\n",
    "model.fit(trn_data, trn_labels)\n",
    "\n",
    "# then classify the test trials\n",
    "class_labels = model.predict(tst_data)\n",
    "\n",
    "# Then just compute our classification accuracy by comparing the predicted labels to the ground truth\n",
    "class_acc = np.sum(class_labels==tst_labels) /  tst_data.shape[0]\n",
    "print('Classification accuracy with a SVM is: ', class_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
