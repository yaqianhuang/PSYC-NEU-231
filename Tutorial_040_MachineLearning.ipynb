{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 04, Intro to machine learning/pattern recognition, Part I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning, or pattern recognition, refers to a large set of tools that are used in many areas of science in any case where you are trying to use multiple measured variables to determine if information is encoded about different levels of an experimental manipulation. \n",
    "* To the extent that there is information in a pattern of measured responses, then the pattern recognition algorithm will be able to successfully assign different examplars into their correct classes.\n",
    "* If there is no information, then it the pattern recognition algorithm will randomly guess and classifation accuracy will be at chance. \n",
    "* Note that the 'pattern' part of 'pattern recognition' refers to the fact that we're not just going to use a single variable to predict our outcome measure - we're going  to use the information encoded by a series of variables to make predictions (i.e. a 'multivariate' analysis).\n",
    "* The multivariate nature of the method is a major advantage as you might infer a null relationship based on univariate methods when in fact there is a very robust relationship that can be revealed by exploiting information encoded in a pattern of measurements. \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Cross-validation** : To assess the generalizability of a pattern classification algorithm. Cross-validation simply refers to the notion that you train your pattern recognition algorithm (henceforth I'll call this a 'classifer') using one set of data, and then you validate, or 'test', the performance of the classifier using a novel set of data that was not part of the training set. The main purpose of cross-validation is to assess the generalizability of your classifier and its ability to correctly categorize novel inputs.\n",
    "\n",
    "This sounds simple but can be tricky...suppose you did an experiment that had 500 trials of stimulus type A and 500 trials of stimulus type B and you measured the response on each trial in 100 neurons. \n",
    "\n",
    "Then, to figure out how well the neural data respond systematically to changes in stimulus parameters, you  fit a multivariate regression analysis to see how much variability in the stimulus is accounted for by changes in neural activity (i.e. you compute something akin to a R^2 value to asses goodness of fit). Suppose you run this analysis on all 1,000 trials and you get your R^2 value and its nice and high - like .75 or so. You might be really happy with this, however, since you fit all of the data in your model, your estimate of how good the model fits the data is almost certainly overestimating how good the model is at accounting for the relationship between the two factors because your dependent variables  (your measured neural responses) are corrupted by noise, and this noise is idiosyncratic in the sense that if you were to perform the experiment again, you'd get 1000 different measurements that were similar to the first 1000, but corrupted by different noise. \n",
    "\n",
    "As a result, when you fit your model to the data, the resulting coeffecients will reflect the true 'signal' in the data AND the idiosyncratic noise that was measured along with the signal. In effect, your model learns the relationship between the independent variable and the (signal + noise). This occurs because your model has no a priori means of separating out signal and noise - it just gets a measure of neural responses that were evoked by each stimulus, and the model is just relating those measurements to the independent variables. This is referred to as 'overfitting', and is a exacerbated by small data sets (where the signal is not likely to emerge from the noise due to the small sample size) and when you have a model that has lots of free parameters (more free parameters means that the model can more flexibly account for random variations in the data...i.e. noise).  \n",
    "So - what to do? Instead of fitting the model to all the data and assessing the goodness of fit, you could use cross-validation to estimate **prediction accuracy**. In our example above, you could train your classifer using 400/500 of the trials associated with each stimulus set (so 800 trials total), and then 'test' the classifer's performance at guessing the correct stimulus class using the remaining 200 trials (100 associated with each stimulus). \n",
    "\n",
    "Then you could permute this train/test procedure several times, each time holding-out a different set of 800 trials to train the classifer and 200 trials to test the performance of the classifier. Here is the cool part: if your model is just learning the idiosyncratic noise in the data, then you might have a reasonable looking R^2 value based on your training data (i.e. the model fits the training data ok), but your ability to classify novel examplars from the test set will be at chance becuase your model just learned the random noise in the training set and there was no consistent 'signal' that could actually discriminate between conditions. \n",
    "\n",
    "So, the use of cross-validation can protect against overly optimistic assessments of model fit due to 'overfitting', and also enables you to assess the generalizability of the model to classify novel exemplars. The degree to which a classifer generalizes to correctly predict novel stimuli is really  then a measure of how much real signal - or information - there is in your data about the different examplars that you're trying to classify.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Out of set prediction**: similar motivation as above with cross-validation, but with out of set prediction you don't repeatedly train and test using folds of the same data set. Instead, you have one set of data that only serves as the training set, and another that serves as the test set (hence the name \"out of set prediction\"). This can be a powerful approach, especially if one training set can be generalized to several test data sets. In practice choosing cross-validation or out of set prediction often depends on how you set up the experiment and if you included a conditions that was specifically designed to serve only as the training set.\n",
    "</div>\n",
    "\n",
    "## Overview - develop intuition for how two common types of pattern recognition algorithms work. In the first tutorial, we'll cover pattern recognition, or 'classification', using the Mahalanobis Distance, and in the second tutorial we'll cover classification using an Support Vector Machine (or SVM). There are TONS of different algorithms and approaches out there, but these are good representative examples\n",
    "* Using a Mahalanobis Distance based classification scheme, we'll model multivariate data from each experimental condition, and then we'll try to assign a class to each 'test' data vector based on how well it fits each of our models.\n",
    "* Using a SVM, we don't fit all the data from each condition, instead we draw a boundary (line, plane, hyperplane) that is maximally far away from the data points in each set that are close to boundary. This is referred to as \"maximizing the margin\"...more on that in the second part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start with imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import eigh\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# also define the default font we'll use for figures. \n",
    "fig_font = {'fontname':'Arial', 'size':'20'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First lets set up some code to generate simulated data\n",
    "* We'll write a function that we can repeatedly call to generate data from two variables measured in two different experimental conditions. \n",
    "* We can also control the degree of correlation between the variables.\n",
    "* For simplicity on function call, some of the variables will be hardcoded and simplified. Can expand this out later if you wanted. \n",
    "* Can see why multivariate analyses are so powerful when comparing a N-D representation to a univariate representation\n",
    "* In this example, we'll have two variables (e.g. neurons), and two experimental conditions\n",
    "* We want to see if the pattern of responses across the two variables systematically varies across conditions\n",
    "* [scipy cookbook for generating correlated samples](https://scipy-cookbook.readthedocs.io/items/CorrelatedRandomSamples.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_data_pnts = number of experimental trials\n",
    "# mu0, mu1 = 2 element np.array that sets the mean of each variable in each condition\n",
    "# cov0, cov1 = symetric 2 x 2 matrix with main diag specifying the variance of each variable \n",
    "# in each condition, and the off diag elements specifying the covariance\n",
    "def gen_cor_data(num_data_pnts, mu0, mu1, cov0, cov1, plot):\n",
    "\n",
    "    # number of variables, in this case lets keep it at 2 because that makes it easy to \n",
    "    # visualize\n",
    "    V = 2\n",
    "\n",
    "    # means of each variable in each condition\n",
    "    mean_of_data0 = mu0; \n",
    "    mean_of_data1 = mu1 \n",
    "\n",
    "    # generate some random data vectors drawn from normal\n",
    "    data0 = np.random.randn(num_data_pnts,V) \n",
    "    data1 = np.random.randn(num_data_pnts,V) \n",
    "\n",
    "    # compute the eigenvalues and eigenvectors of the cov matrix\n",
    "    evals0, evecs0 = eigh(cov0)\n",
    "    evals1, evecs1 = eigh(cov1)\n",
    "\n",
    "    # Construct c, so c*c^T = cov.\n",
    "    c0 = np.dot(evecs0, np.diag(np.sqrt(evals0)))\n",
    "    c1 = np.dot(evecs1, np.diag(np.sqrt(evals1)))\n",
    "\n",
    "    # convert the data using by multiplying data by c\n",
    "    # to be consistent with previous tutorials, we want the data running down columns...so do the double .T\n",
    "    cdata0 = np.dot(c0, data0.T).T\n",
    "\n",
    "    # then add in the mu offset...use np.ones * each condition mean\n",
    "    cdata0 += np.hstack((np.ones((num_data_pnts,1))*mean_of_data0[0], np.ones((num_data_pnts,1))*mean_of_data0[1]))\n",
    "\n",
    "    # repeat for the data from the second experimental condition \n",
    "    cdata1 = np.dot(c1, data1.T).T\n",
    "    cdata1 += np.hstack((np.ones((num_data_pnts,1))*mean_of_data1[0], np.ones((num_data_pnts,1))*mean_of_data1[1])) \n",
    "\n",
    "    if plot:\n",
    "        # plot the data...\n",
    "        plt.scatter(cdata0[:,0], cdata0[:,1], color='b')\n",
    "        plt.scatter(cdata1[:,0], cdata1[:,1], color='r')\n",
    "        plt.xlabel('Resp Neuron 1', **fig_font)\n",
    "        plt.ylabel('Resp Neuron 2', **fig_font)\n",
    "        plt.legend(['Condition 1', 'Condition 2'])\n",
    "        plt.show()\n",
    "    \n",
    "    return cdata0, cdata1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed the random num sequence so that we get consistent results every time!\n",
    "np.random.seed(2)\n",
    "\n",
    "# try out our function...\n",
    "# mu0 is the mean response in each neuron under the first experimental condition\n",
    "# mu1 is the mean response in each neuron under the second experimental condition\n",
    "mu0 = np.array([1,4])\n",
    "mu1 = np.array([4,1])\n",
    "\n",
    "# specify covariance matrices for responses under each condition...for now lets \n",
    "# just keep them the same, with variance 2 and cov=0 (which means that the responses\n",
    "# of the two neurons are independent)\n",
    "cov0 = np.array([\n",
    "        [2, 0],\n",
    "        [0, 2]\n",
    "    ])\n",
    "\n",
    "cov1 = np.array([\n",
    "        [2, 0],\n",
    "        [0, 2]\n",
    "    ])\n",
    "\n",
    "# how many data points do we want to generate for each condition?\n",
    "N = 100\n",
    "\n",
    "# now call our data generating function and return data from each condition in d0 and d1\n",
    "d0, d1 = gen_cor_data(num_data_pnts=N, mu0=mu0, mu1=mu1, cov0=cov0, cov1=cov1, plot=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A few things to note about this data set - univariate vs multivariate\n",
    "* You can clearly see that these neurons respond differently under our two experimental conditions\n",
    "* However, if you were to average across both neurons and report univariate summary statistics, there would be NO difference between conditions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same response (within noise given 100 random samples...numbers would converge with higher N)\n",
    "print('Mean response in condition 1 across neurons: ', np.mean(d0[:]))\n",
    "print('Mean response in condition 2 across neurons: ', np.mean(d1[:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To exploit the fact that we have a clearly different pattern of responses across neurons, we can:\n",
    "* First split the data into 'train' and 'test' sets\n",
    "* Model the data in each condition using data ONLY from the training set\n",
    "* For each trial of data in the test set, we can classify it as belonging to one condition or the other. \n",
    "\n",
    "## How do actually make the assignment of a test data vector to a class (or experimental condition)?\n",
    "* The intuition is that we need to figure out how far each test vector is from the 'cloud' of data points associated with each condition)\n",
    "* Simplest way: compute the mean of each data set, or cloud of points in our 2D example, and then compute the Euclidean distance between each test vector, which is a single point in our 2D data set, and each of the condition means\n",
    "* Classify the test vector based on the minimum distance to each training set mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some data, just like above\n",
    "np.random.seed(2)\n",
    "d0, d1 = gen_cor_data(num_data_pnts=N, mu0=mu0, mu1=mu1, cov0=cov0, cov1=cov1, plot=0)\n",
    "\n",
    "# now lets generate a training set and a test set. We'll use 90% of our data as a training set\n",
    "# and leave 10% as a test set\n",
    "r,c = d0.shape  # get the shape of our data set (and d1 is the same size, so just need to get one of these)\n",
    "trn_length = int(np.round(.9*r, 0))\n",
    "\n",
    "# now assign the first 90% of the trials to the training set...\n",
    "# we'll stack up the first 90% from the first condition on top \n",
    "# of the 90% of data from the second condition\n",
    "trn_data = np.vstack((d0[0:trn_length,:], d1[0:trn_length,:]))\n",
    "\n",
    "# and the last 10% of the trials to the test set...\n",
    "tst_data = np.vstack((d0[trn_length:,:], d1[trn_length:,:]))\n",
    "\n",
    "# last its helpful to generate a vector that labels the data from each trial\n",
    "# as belonging to condition 1 or condition 2\n",
    "trn_labels = np.hstack((np.zeros(trn_length), np.ones(trn_length)))\n",
    "tst_labels = np.hstack((np.zeros(r-trn_length), np.ones(r-trn_length)))\n",
    "\n",
    "# compute the mean of the data in each training set in the 2D space\n",
    "mean0 = np.mean(trn_data[trn_labels==0,:], axis=0)\n",
    "mean1 = np.mean(trn_data[trn_labels==1,:], axis=0)\n",
    "print('Mean of first cond: ', mean0, ' Mean of second cond: ', mean1)\n",
    "\n",
    "# ok, now lets just compute the distance between each test vector (2D point) and each condition mean\n",
    "# first we'll do it the longer way with a loop over test trials\n",
    "\n",
    "# allocate an array to store our classification labels  \n",
    "class_labels = np.zeros(tst_data.shape[0]) \n",
    "for i in np.arange(tst_data.shape[0]):\n",
    "    # compute Euc distance from each point to the mean of first condition\n",
    "    # sqrt of sum if squared distances...\n",
    "    distance0 = np.sqrt( np.sum( np.power( (tst_data[i,:]-mean0) , 2) ) )\n",
    "    distance1 = np.sqrt( np.sum( np.power( (tst_data[i,:]-mean1) , 2) ) )\n",
    "    # then figure out which distance is smaller (or which cloud our test point is closer to)\n",
    "    class_labels[i] = np.argmin([distance0, distance1])\n",
    "    \n",
    "# Then just compute our classification accuracy by comparing the predicted labels to the ground truth\n",
    "class_acc = np.sum(class_labels==tst_labels) /  tst_data.shape[0]\n",
    "print('Classification accuracy is: ', class_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note that in practice, we would repeatedly hold out a different set of 10% of the data as a test set and use the remaining 90% as a training set (i.e. cross-validation with many folds).\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "While you can do as many folds of cross-validation as you want, so long as the train and test data are independent, always ensure that you have a balanced training set that has an equal number of trials from each of your conditions! If you don't, your classifer will generally be biased towards the condition with the most examples. This can lead to insidious above chance classification if your test set also has the same imbalance in trials in each condition!!!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So the Euc distance approach works pretty well, but all we're doing is computing the distance from each test vector to the center of mass of each training distribution. \n",
    "* Can also weight the distance by the variance of the training set distributions\n",
    "* Intuition: if a variable is very reliable (low variance), then we want to \"trust\" the distance more...i.e. we want to weight the final distance metric such that its biased in favor of the more reliable neurons.\n",
    "* Referred to Normalized Euclidean Distance\n",
    "\n",
    "[pooled variance formula if you want to look up...](https://en.wikipedia.org/wiki/Pooled_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start here just like the last cell - only the computation of the metric is different.\n",
    "# however, for fun, lets make one of our neurons super noisy\n",
    "np.random.seed(2)\n",
    "var_of_neuron0 = 20\n",
    "var_of_neuron1 = 1\n",
    "\n",
    "cov0 = np.array([\n",
    "        [var_of_neuron0, 0],\n",
    "        [0, var_of_neuron1]\n",
    "    ])\n",
    "\n",
    "cov1 = np.array([\n",
    "        [var_of_neuron0, 0],\n",
    "        [0, var_of_neuron1]\n",
    "    ])\n",
    "\n",
    "# generate the data and plot...\n",
    "d0, d1 = gen_cor_data(num_data_pnts=N, mu0=mu0, mu1=mu1, cov0=cov0, cov1=cov1, plot=1)\n",
    "\n",
    "# now lets generate a training set and a test set. We'll use 90% of our data as a training set\n",
    "# and leave 10% as a test set\n",
    "r,c = d0.shape  # get the shape of our data set (and d1 is the same size, so just need to get one of these)\n",
    "trn_length = int(np.round(.9*r, 0))\n",
    "\n",
    "# now assign the first 90% of the trials to the training set...\n",
    "# we'll stack up the first 90% from the first condition on top \n",
    "# of the 90% of data from the second condition\n",
    "trn_data = np.vstack((d0[0:trn_length,:], d1[0:trn_length,:]))\n",
    "\n",
    "# and the last 10% of the trials to the test set...\n",
    "tst_data = np.vstack((d0[trn_length:,:], d1[trn_length:,:]))\n",
    "\n",
    "# last its helpful to generate a vector that labels the data from each trial\n",
    "# as belonging to condition 1 or condition 2\n",
    "trn_labels = np.hstack((np.zeros(trn_length), np.ones(trn_length)))\n",
    "tst_labels = np.hstack((np.zeros(r-trn_length), np.ones(r-trn_length)))\n",
    "\n",
    "# compute the mean of the data in each training set in the 2D space\n",
    "mean0 = np.mean(trn_data[trn_labels==0,:], axis=0)\n",
    "mean1 = np.mean(trn_data[trn_labels==1,:], axis=0)\n",
    "\n",
    "# This part is just a little bit different from above\n",
    "# we'll do the same basic sum of sqrt of squared differences, but we'll weigth by \n",
    "# the variance of each variable so that more reliable neurons contribute more \n",
    "# to our distance metric!\n",
    "# So lets compute pooled variance (variance of each condition weighted by number of data \n",
    "# points in each condition)\n",
    "numerator = ((trn_length-1) * np.var(trn_data[trn_labels==0,:], axis=0) + \n",
    "    (trn_length-1) * np.var(trn_data[trn_labels==1,:], axis=0))\n",
    "\n",
    "denom = (trn_length-1)+(trn_length-1)\n",
    "\n",
    "# get the variance of each neuron across both conditions \n",
    "pooled_variance = numerator / denom\n",
    "\n",
    "# allocate an array to store our classification labels  \n",
    "class_labels = np.zeros(tst_data.shape[0]) \n",
    "# and to store our Euc based classifer results for comparison\n",
    "class_labels_old = np.zeros(tst_data.shape[0]) \n",
    "\n",
    "for i in np.arange(tst_data.shape[0]):\n",
    "    # compute Norm Euc distance from each point to the mean of each condition\n",
    "    distance0 = np.sqrt( np.sum( np.power( (tst_data[i,:]-mean0) / pooled_variance , 2) ) )\n",
    "    distance1 = np.sqrt( np.sum( np.power( (tst_data[i,:]-mean1) / pooled_variance , 2) ) )\n",
    "    # then figure out which distance is smaller (or which cloud our test point is closer to)\n",
    "    class_labels[i] = np.argmin([distance0, distance1])\n",
    "\n",
    "    # compute the plain old Euc distance based classifer for comparison\n",
    "    distance0 = np.sqrt( np.sum( np.power( (tst_data[i,:]-mean0), 2) ) )\n",
    "    distance1 = np.sqrt( np.sum( np.power( (tst_data[i,:]-mean1), 2) ) )\n",
    "    # then figure out which distance is smaller (or which cloud our test point is closer to)\n",
    "    class_labels_old[i] = np.argmin([distance0, distance1])    \n",
    "    \n",
    "# Then just compute our classification accuracy by comparing the predicted labels to the ground truth\n",
    "class_acc = np.sum(class_labels==tst_labels) /  tst_data.shape[0]\n",
    "print('Classification accuracy with Norm Euc Dis is: ', class_acc)\n",
    "\n",
    "class_acc_old = np.sum(class_labels_old==tst_labels) /  tst_data.shape[0]\n",
    "print('Classification accuracy with PLain Old Euc Dis is: ', class_acc_old)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now go back and run that last cell with var_of_neuron0 set to 20 (or larger). What happens when you compare the output using the norm euc distance to the output from just the euc distance?\n",
    "* This approach is especially helpful when your variables have unequal variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note that both of the above examples considered the case of independent (non-correlated) variables. \n",
    "* In other words, the response of neuron0 is not systematically related to the response of neuron1\n",
    "* However, when there are correlations, the Normalized Euc distance metric can sometimes run into problems\n",
    "* To see why we can just look at a cloud of points in a data set generated using correlated variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this time lets explictly define the variance and covariance structure of the data\n",
    "# to start we'll keep the variance relatively low and just mess with the cov\n",
    "np.random.seed(2)\n",
    "mu0 = np.array([1,4])\n",
    "mu1 = np.array([4,1.5])\n",
    "\n",
    "# variance of each neuron\n",
    "var_of_neuron0 = 3\n",
    "var_of_neuron1 = 3\n",
    "\n",
    "# covariance of neurons in each experimental condition \n",
    "cov_cond_0 = 2.5\n",
    "cov_cond_1 = 2.5\n",
    "\n",
    "cov0 = np.array([\n",
    "        [var_of_neuron0, cov_cond_0],\n",
    "        [cov_cond_0, var_of_neuron1]\n",
    "    ])\n",
    "\n",
    "cov1 = np.array([\n",
    "        [var_of_neuron0, cov_cond_1],\n",
    "        [cov_cond_1, var_of_neuron1]\n",
    "    ])\n",
    "\n",
    "# generate the data and plot...\n",
    "d0, d1 = gen_cor_data(num_data_pnts=N, mu0=mu0, mu1=mu1, cov0=cov0, cov1=cov1, plot=0)\n",
    "\n",
    "# Plot the data\n",
    "plt.scatter(d0[:,0], d0[:,1], color='b')\n",
    "plt.scatter(d1[:,0], d1[:,1], color='r')\n",
    "plt.xlabel('Resp Neuron 1', **fig_font)\n",
    "plt.ylabel('Resp Neuron 2', **fig_font)\n",
    "# then I'm going to pick one data point in this set to demonstrate the problem that can arise\n",
    "plt.scatter(d0[1,0], d0[1,1], color='g', linewidths=10)\n",
    "plt.legend(['Condition 1', 'Condition 2','Data pnt from C2'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So the problem here is that the green point is actually a little bit closer to the center of the Condition 2 than to the center of Condition 1 (even though its part of data set from Condition 1)...\n",
    "* To fix this we need to not only account for the variance of each neuron (variable), but also account for the covariance structure of the data!\n",
    "* Actually just a simple extension of the Normalized Euc Distance metric, called the Mahalonobis distance\n",
    "* Its the Euc Distance weighted by the covariance structure\n",
    "* Normalizing by the cov matrix has the effect of undoing the correlations between variables!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how it works...just need data from one condition to make the point\n",
    "plt.title('Correlated data')\n",
    "plt.scatter(d0[:,0], d0[:,1], color='b')\n",
    "plt.xlabel('Resp Neuron 1', **fig_font)\n",
    "plt.ylabel('Resp Neuron 2', **fig_font)\n",
    "plt.legend(['Condition 1'])\n",
    "plt.show()\n",
    "\n",
    "# now normalize by the covariance structure\n",
    "# first estimate cov matrix\n",
    "# rowvar = false to specify that each row is a measurement and each column is a variable\n",
    "# note that rowvar is TRUE by default!!! so if you have a #trials x #variable data set\n",
    "# this will return a #var x # var matrix by default...and that might not be what you want...\n",
    "cm = np.cov(d0,rowvar=0)\n",
    "\n",
    "# \"whiten\" data by matrix multiplying by inverse of covariance matrix\n",
    "# note the @ operator, which is same as np.dot but much cleaner to write sometimes...\n",
    "d0_white = d0 @ np.linalg.inv(sqrtm(cm))\n",
    "\n",
    "# replot\n",
    "plt.title('Correlation is gone!')\n",
    "plt.scatter(d0_white[:,0], d0_white[:,1], color='b')\n",
    "plt.xlabel('Resp Neuron 1', **fig_font)\n",
    "plt.ylabel('Resp Neuron 2', **fig_font)\n",
    "plt.legend(['Condition 1'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now put this all together and do classification based on the Maha distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make our data set...then use the Norm Euc distance and the Maha distance to classify.\n",
    "np.random.seed(2)\n",
    "mu0 = np.array([1,4])\n",
    "mu1 = np.array([4,1.5])\n",
    "\n",
    "# variance of each neuron\n",
    "var_of_neuron0 = 3\n",
    "var_of_neuron1 = 3\n",
    "\n",
    "# covariance of neurons in each experimental condition \n",
    "cov_cond_0 = 2\n",
    "cov_cond_1 = 2\n",
    "\n",
    "cov0 = np.array([\n",
    "        [var_of_neuron0, cov_cond_0],\n",
    "        [cov_cond_0, var_of_neuron1]\n",
    "    ])\n",
    "\n",
    "cov1 = np.array([\n",
    "        [var_of_neuron0, cov_cond_1],\n",
    "        [cov_cond_1, var_of_neuron1]\n",
    "    ])\n",
    "\n",
    "# generate the data and plot...\n",
    "d0, d1 = gen_cor_data(num_data_pnts=N, mu0=mu0, mu1=mu1, cov0=cov0, cov1=cov1, plot=1)\n",
    "\n",
    "# now lets generate a training set and a test set. We'll use 90% of our data as a training set\n",
    "# and leave 10% as a test set\n",
    "r,c = d0.shape  # get the shape of our data set (and d1 is the same size, so just need to get one of these)\n",
    "trn_length = int(np.round(.9*r, 0))\n",
    "\n",
    "# now assign the first 90% of the trials to the training set...\n",
    "# we'll stack up the first 90% from the first condition on top \n",
    "# of the 90% of data from the second condition\n",
    "trn_data = np.vstack((d0[0:trn_length,:], d1[0:trn_length,:]))\n",
    "\n",
    "# and the last 10% of the trials to the test set...\n",
    "tst_data = np.vstack((d0[trn_length:,:], d1[trn_length:,:]))\n",
    "\n",
    "# last its helpful to generate a vector that labels the data from each trial\n",
    "# as belonging to condition 1 or condition 2\n",
    "trn_labels = np.hstack((np.zeros(trn_length), np.ones(trn_length)))\n",
    "tst_labels = np.hstack((np.zeros(r-trn_length), np.ones(r-trn_length)))\n",
    "\n",
    "# compute the mean of the data in each training set in the 2D space\n",
    "mean0 = np.mean(trn_data[trn_labels==0,:], axis=0)\n",
    "mean1 = np.mean(trn_data[trn_labels==1,:], axis=0)\n",
    "\n",
    "# Compute pooled variance so that we can compute the Norm distance and\n",
    "# use as a comparison point for how the Maha distance metric does\n",
    "numerator = ((trn_length-1) * np.var(trn_data[trn_labels==0,:], axis=0) + \n",
    "    (trn_length-1) * np.var(trn_data[trn_labels==1,:], axis=0))\n",
    "\n",
    "denom = (trn_length-1)+(trn_length-1)\n",
    "\n",
    "# get the variance of each neuron across both conditions \n",
    "pooled_variance = numerator / denom\n",
    "\n",
    "# Now compute the pooled covariance estimate...use this with the maha distance metric\n",
    "numerator = ((trn_length-1) * np.cov(trn_data[trn_labels==0,:], rowvar=0) + \n",
    "    (trn_length-1) * np.cov(trn_data[trn_labels==1,:], rowvar=0))\n",
    "\n",
    "denom = (trn_length-1)+(trn_length-1)\n",
    "\n",
    "# compute pooled covar\n",
    "pooled_covariance = numerator / denom\n",
    "\n",
    "# allocate an array to store our classification labels  \n",
    "class_labels = np.zeros(tst_data.shape[0]) \n",
    "# and to store our Euc based classifer results for comparison\n",
    "class_labels_norm = np.zeros(tst_data.shape[0]) \n",
    "\n",
    "for i in np.arange(tst_data.shape[0]):\n",
    "    # use the maha distance to compute our classification \n",
    "    # we'll use a slightly different notation here in a more linear algebra friendly way...\n",
    "    # this is the same as the sum of squared distances weighted (divided, or multipled by inverse) by the cov matrix\n",
    "    distance0 = np.sqrt((tst_data[i,:]-mean0) @ np.linalg.inv(pooled_covariance) @ (tst_data[i,:]-mean0).T)\n",
    "    distance1 = np.sqrt((tst_data[i,:]-mean1) @ np.linalg.inv(pooled_covariance) @ (tst_data[i,:]-mean1).T)\n",
    "    class_labels[i] = np.argmin([distance0, distance1]) \n",
    "\n",
    "    # compute Norm Euc distance from each point to the mean of each condition for comparison\n",
    "    distance0 = np.sqrt( np.sum( np.power( (tst_data[i,:]-mean0) / pooled_variance , 2) ) )\n",
    "    distance1 = np.sqrt( np.sum( np.power( (tst_data[i,:]-mean1) / pooled_variance , 2) ) )\n",
    "    # then figure out which distance is smaller (or which cloud our test point is closer to)\n",
    "    class_labels_norm[i] = np.argmin([distance0, distance1]) \n",
    "    \n",
    "# Then just compute our classification accuracy by comparing the predicted labels to the ground truth\n",
    "class_acc = np.sum(class_labels==tst_labels) /  tst_data.shape[0]\n",
    "print('Classification accuracy with Maha Dis is: ', class_acc)\n",
    "\n",
    "class_acc_norm = np.sum(class_labels_norm==tst_labels) /  tst_data.shape[0]\n",
    "print('Classification accuracy with Norm Euc Dis is: ', class_acc_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So when we have correlated variables, the Maha distance might be preferred over the Norm Euc distance model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "* All of the pattern recognition algorithms presented here rely on modeling the data associated with each condition and then determining some kind of fit between each 'test' vector (pattern) and the model for each condition in the training set.\n",
    "* One of the simplest approaches is to just model the center of mass for each condition, and then compute the difference between each test vector and each center of mass...assign each test vector based on the min distance to each center of mass.\n",
    "* This is fine if you have uncorrelated variables and each variable has equal variance. \n",
    "* If you have unequal variance, then you should also include an appropriate normilization factor. \n",
    "* To do so, you can normalize the Euc Distance by the variance of each variable, then you get the Normalized Euc distance, which is often a more suitable measure.\n",
    "* If you have correlated variables (with or without equal variance) then you should consider the Mahalonobis Distance, which accounts for both the variance and covariance structure of your data. \n",
    "* Use randomization testing to eval significance...repeatedly classify with shuffled condition labels in the training set to generate a null distribution of classification accuracies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
