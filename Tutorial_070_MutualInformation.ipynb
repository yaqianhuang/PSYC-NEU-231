{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy, conditional entropy, and mutual information\n",
    "\n",
    "Overview:\n",
    "* In this tutorial, we will learn how to describe the information that is shared between two variables (mutual information). In other words, how much uncertainty reduction is there to be had about variable 1 by measuring variable 2? \n",
    "\n",
    "* These concepts were initially developed in communication theory to describe the efficacy of transmitting signals over a noisy medium (like a noisy telephone line). For example, suppose that we want to know how good a communication channel is, or its effeciency in reliably relaying a message from point A (a 'sender') to point B (a 'reciever').\n",
    "\n",
    "* Basically this is just like asking, \"we know how good the signal is at A, and we recieved the message at B - how much information about A is still in the received signal B?\". \n",
    "\n",
    "* So that is the general gist of it, but right away you can see the potential applicability of this metric in many fields of neuroscience, psychology, engeneering, etc. In neuroscience, we're dealing with a series of communication channels that are corrupted by noise (e.g. synapses). It is therefore reasonable to ask: how much information from neuron A effectively propogates to neuron B? (or conversely, how much  information is lost?).\n",
    "\n",
    "* However, this logic works for any combination of variables: two continuous variables, two discrete variables, one continuous and one discrete, etc. As a result, we can ask questions about any two variables really: how much information about median home  price is reflected in stock market fluctuations? etc.\n",
    "\n",
    "* A few notes before we get started. First, we're going to be talking a lot about 'uncertainty' and 'uncertainty reduction'. While this is basically complementary to talking about certainty and an increase in certainty, we'll deal with the former terminology as it is embedded in some of the concepts that we'll discuss. \n",
    "\n",
    "* Second, we'll be dealing with variability in data, and how we can either attribute that variability in the data to 'noise' or to 'signals'. I.e. is the variability in one variable random wrt another variable? or does the variability in one variable systematically change with the variability in another? \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Finally, a lot of people think at this point, \"why not just correlate the variables using a normal r-value?\". There are a few answers to this, but the simplest is this: correlation assumes a linear relationship (or, in more complex forms, a known relationship or you have to assume a relationship) between variables. Mutual information does not, and can generally capture any form of linear or non-linear relationship between two variables. This makes it a very powerful and general purpose metric.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# also define the default font we'll use for figures. \n",
    "fig_font = {'fontname':'Arial', 'size':'20'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART II: Entropy as a measure of variability\n",
    "\n",
    "* Shannon Entropy is related to the variability of data, but is more specifically defined as the average uncertainty in a set of measurements \n",
    "* Consider coin tosses - lets say we have a balanced coin, and we flip it once. We can represent the outcome of a single toss as a 0 or a 1 (a head or a tail), and this has an entropy (uncertainty) of 1 bit. In other words, we would reduce our uncertainty completely after we observed the outcome, and that would correspond to a reduction of 1 bit. \n",
    "    * By extension, if we flipped the coin twice, then the entropy would be 2 bits (00, 01, 10, or 11) because we would reduce our uncertainty by 2 bits after we observed the outcome. \n",
    "\n",
    "* To keep with the coin analogy...when the coin is balanced and heads and tails are equally likely, then the entropy is highest because uncertainty is maximized and flipping the coin will give you 1 bit of information.  \n",
    "* To see why entropy is maximized in this situation, consider a biased coin  that comes up heads 60% the time. \n",
    "    * In this case, we could predict the outcome of the coin flip better than chance simply by going with our prior of 'heads'. Thus, the entropy assoicated with a biased coin is less than the entropy associated with the unbiased coin, because the reduction in uncertainty is lower with the biased coin than with the unbaised coin. \n",
    "    * We learn less after flipping the biased coin than we do when we flip the unbiased coin. \n",
    "* This example brings up an important point: entropy as a measure of uncertainty is maximized when all possible outcomes are equally likely because you have no prior info upon which to make an educated guess about the outcome (i.e. a uniform distribution on the span of possible outcomes has the highest entropy). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To figure out the entropy of a measurement in terms of bits (the most common metric), we can use the log2 function, referred to as the binary logarithm and the inverse function of 2^N. The log2(n) is the power to which the number 2 must be raised to obtain the value n. Lets go back to our coin flip example with a fair coin. Suppose you flipped the coin once - the entropy would be 1 because there are two possible outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log2(2) # log2(n) or 2^x = n? ...x = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  If you flipped the coin twice you'd have 4 possible outcomes (00,10,01,11), or three times you'd have 8 possible outcomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.log2(4)) # or 2^x = n, x = 2\n",
    "print(np.log2(8)) # or 2^x = n, x = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To see the general shape of the function, plot out log2(x:y) to see the relationship between the number of possible outcomes and the entropy in bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# why am i going from 1,15 instead of our usual 0 based counting?\n",
    "plt.plot(np.arange(1,15), np.log2(np.arange(1,15)), linewidth=3)\n",
    "plt.ylim([-1,5])\n",
    "plt.xticks(**fig_font)\n",
    "plt.yticks(**fig_font)\n",
    "plt.ylabel('Entropy (2^N = ?)', **fig_font)\n",
    "plt.xlabel('# of possible outcomes', **fig_font)\n",
    "plt.show()\n",
    "# note that log2(0) == -inf, and that log2(1)==0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going back to the above notion that entropy is maximized when the outcome is completely uncertain (e.g. a fair coin), then we can start to develop an intuition about expressing entropy in terms of the probability that some event will occur, denoted P(X). \n",
    "* Lets take a more complex case - lets say that we have a slot machine that has two wheels on it and the first wheel can take one of N states and the second can take one of M states  when we pull the handle. Considering just the first wheel, the possible outcomes are {x1....xn}, and if each outcome is equally likely, then p(xi)= 1/n.  So, for example, if n = 16, then the total entropy of wheel 1 can be represented  by 4 bits of information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log2(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now what about the second wheel? It can take on M states. So inutively the total number of possible outcomes for both wheels is N * M. The uncertainty of the outcome in this case is then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 16;\n",
    "m = 16;\n",
    "print(np.log2(n*m))\n",
    "\n",
    "# which recall is equal to \n",
    "print(np.log2(n) + np.log2(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which gives us a nice way to account for the probability of outcomes across multiple variables via summation! Why is this computationally convienent, esp on digital computers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So in this case we need 8 bits of information to specify all possible outcomes. Now lets consider each possible outcome in isolation. what is the uncertainty associated with each event? its the probability of that event occuring, which is p(xi) = 1/n. So in terms of bits, we have :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 16;\n",
    "-np.log2(1/n)  #why negative when dealing with probabilities? log2(1)-log2(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we'd like though is a way to assess the average uncertainty of a particular outcome across all possible outcomes. How would you do that? \n",
    "* You'd take the uncertainty of each outcome (-log2(p(xi)) and weight it by the probability that the event will actually  occur, like this (where entropy is denoted, by convention, as H):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H = -sum_over_all_i( p(xi) * log2(p(xi)) )\n",
    "n = 16;\n",
    "H = 0;\n",
    "for i in np.arange(n):\n",
    "    H += -( (1/n) * np.log2(1/n) )\n",
    "\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does H, the average uncertaintly, equal when all events are equally likely? it equals the uncertaintly of each event, which it should. And note that this framework would let you compute the average uncertainty (entropy) across both slot machine wheels by just adding the entropies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 16 # number of possible outcomes for machine 1\n",
    "m = 16 # number of possible outcomes for machine 2\n",
    "H = 0\n",
    "for i in np.arange(n):\n",
    "    H += -( (1/n) * np.log2(1/n) )\n",
    "\n",
    "for i in np.arange(m):\n",
    "    H += -( (1/m) * np.log2(1/m) )\n",
    "\n",
    "print(H)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about when all events are not equally likely?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember that with 16 possible equally likely outcomes\n",
    "# we should have an entropy of 4\n",
    "n = 16\n",
    "\n",
    "# pick some random values to use for p(x)\n",
    "px = np.random.rand(n)\n",
    "px /= np.sum(px) # convert the random numbers to probabilities\n",
    "\n",
    "H = 0\n",
    "for i in np.arange(n):\n",
    "    H += -( px[i] * np.log2(px[i]) )\n",
    "\n",
    "print(H)\n",
    "\n",
    "plt.plot(np.arange(n),px)\n",
    "plt.xlabel('Outcome', **fig_font)\n",
    "plt.ylabel('P(outcome)', **fig_font)\n",
    "plt.title('Prob of each of N outcomes', **fig_font)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Note that the entropy is lower because we've moved away from the point where everything is maximally unpredictable (i.e. a uniform distribution). This demonstrates a principle that relates probability distributions to entropy: uniform probability distributions have maximum entropy, and non-uniform distributions will generally have less entropy because some outcomes are more likely than others so you learn less by making a measurement.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of outcomes (e.g. heads or tails)\n",
    "n = 2  \n",
    "\n",
    "# vary the probability of heads\n",
    "ph = np.arange(.001,.999,.001)  \n",
    "\n",
    "# alloc to store entropy at each probability\n",
    "H = np.zeros(len(ph))\n",
    "\n",
    "for j in np.arange(len(ph)):\n",
    "    px[0] = ph[j]    # p(heads)\n",
    "    px[1] = 1-px[0]  # p(tails))\n",
    "    \n",
    "    # then compute entropy\n",
    "    for i in np.arange(n):\n",
    "        H[j] += -( px[i] * np.log2(px[i]) )\n",
    "    \n",
    "\n",
    "plt.plot(ph, H, 'k', linewidth=2)\n",
    "plt.xlabel('Probability of heads (coin bias)', **fig_font)\n",
    "plt.ylabel('entropy (bits)', **fig_font)\n",
    "plt.axvline(.5, color='k', linewidth=2)\n",
    "plt.show()\n",
    "# entropy is maximized with maximum uncertainty and will max out at\n",
    "# log2(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUMMARY\n",
    "* Entropy is a measure of uncertainty, and as uncertainty goes up (and is maximized when all outcomes are equally likely) then the information gained by making a measurement goes up. \n",
    "* So if you know the oucome in advance (e.g. a coin with two heads) then p(tails) = 0 and there is no uncertainty, entropy is 0, and there is no reduction in uncertainty to be gained by flipping the coin at all. \n",
    "* If you have a fair coin, then p(head)==p(tail) and entropy will be maximum and you will maximally reduce your uncertainty by making the measurement (in this case, you will fully disambiguate the outcome, gaining 1 bit of information where the total uncertainty is 1 bit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual information (MI). \n",
    "\n",
    "* MI is a measure of how much knowing  about 1 variable tells you about the state of another variable. Putting aside entropy and measures of uncertainty/variance for a minute, here is the  intuition. \n",
    "    * Suppose you have two variables that are completely unrelated to each other: measuring one variable will tell you nothing about the state of the other variable. \n",
    "    * In contrast, if you have two variables that are perfectly correlated, then measuring one variable will tell you everything about the state of the other. \n",
    "    * In this special (unusual) case, the mutual information will be equal to the entropy of either variable alone (that is: the information gained by measuring one variable will be equal to the information gained by measuring both)\n",
    " \n",
    "To put this back in terms of entropy: lets say we have two variables, X and Y. If we want to assess the MI between X and Y, then we need to know the following difference score:\n",
    "\n",
    "(total entropy of X) -  (entropy of X given that we know Y). \n",
    "\n",
    "In other words, how much is uncertainty about X REDUCED when we measure Y? That is the MI between the two variables. And it leads to one common definition of MI:\n",
    "\n",
    "MI = H(X) - H(X|Y)  \n",
    "\n",
    "where H(X) is the entropy of X, and H(X|Y) is the conditional entropy of X given that we've measured Y - it is the average entropy of X across all values of Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two discrete, uncorrelated arrays filled with 0's and 1's\n",
    "\n",
    "N = 1000   # number of data points\n",
    "x = np.round(np.random.rand(N))\n",
    "y = np.round(np.random.rand(N))\n",
    "\n",
    "px = np.zeros(2)\n",
    "px[0] = np.sum(x)/N   # probability that x==1\n",
    "px[1] = 1-px[0];      # prob that x==0\n",
    "\n",
    "# do in one line instead of looping using the * operator\n",
    "Hx = -sum( px * np.log2(px) )\n",
    "\n",
    "# then compute average conditional entropy of x given y (Hxy).\n",
    "# 1) Compute the entropy of X given each possible value of Y\n",
    "# 2) Multiply H(X|Yi) with the probability of each Y (i.e. p(yi))\n",
    "# 3) Sum H(X|Yi) over all i\n",
    "\n",
    "# initialize Hxy\n",
    "Hxy=0\n",
    "\n",
    "# figure out the unique values in each vector (we know that its 0/1, but do this just for good practice)\n",
    "uniquex = np.unique(x)\n",
    "uniquey = np.unique(y)\n",
    "\n",
    "# loop over unique elements of y, in this case 0,1\n",
    "for i in np.arange(len(uniquey)): \n",
    "    \n",
    "    # probability that y==y(i) (prob of each y)\n",
    "    py = np.sum(y==uniquey[i]) / N\n",
    "\n",
    "    # then loop over all possible x's to compute entropy of x at each y\n",
    "    tmp=0\n",
    "    for j in np.arange(len(uniquex)):\n",
    "        px_y = np.sum((x==uniquex[j]) & (y==uniquey[i])) / np.sum(y==uniquey[i])    # e.g. prob x==1 when y==0\n",
    "        tmp += (-( px_y * np.log2(px_y) ))                                                 # entropy      \n",
    "        \n",
    "    # then tally up entropy of x given each specific y multiplied by the probability of that y (py)\n",
    "    Hxy += py*tmp\n",
    "\n",
    "# then we have everything we need to compute MI, which in this case should\n",
    "# be ~0 becuase the variables are completely independent!\n",
    "MI = Hx - Hxy\n",
    "print(MI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that we've walked all the way through one example, lets define a few functions to make things easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(x):\n",
    "    \"\"\"compute entropy of discrete array x\n",
    "\n",
    "    Args:\n",
    "        x (int): array of discrete values\n",
    "\n",
    "    Returns:\n",
    "        Hx (float): entropy of x\n",
    "\n",
    "    \"\"\"\n",
    "    # figure out unique values of x - can be more than just 0s, 1s\n",
    "    uniquex = np.unique(x)\n",
    "\n",
    "    Hx = 0\n",
    "    for i in np.arange(len(uniquex)):\n",
    "        # probability that x==uniquex[i]\n",
    "        px = np.sum(x==uniquex[i])/len(x)    \n",
    "\n",
    "        # check for px==0 because log2(0) = -inf\n",
    "        if px!=0:\n",
    "            Hx += (-np.sum( px * np.log2(px) ))  \n",
    "        else:\n",
    "            print('px is zero for value ', i)\n",
    "        \n",
    "    return Hx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def condEntropy(x,y):\n",
    "    \n",
    "    \"\"\"\n",
    "    conditional entropy, or the average entropy of x given each y, or Hxy\n",
    "    1) For all Y {i=1:numel(X)}, compute the entropy of X given each Y\n",
    "    2) Multiply H(X|Y==i) with the probability of each Y (i.e. pxi)\n",
    "    3) Sum over all i\n",
    "\n",
    "    Args:\n",
    "        x (int): array of discrete values\n",
    "        y (int): array of discrete values\n",
    "        \n",
    "    Returns:\n",
    "        Hxy (float): average conditional entropy of x given y\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Hxy=0\n",
    "    uniquex = np.unique(x)\n",
    "    uniquey = np.unique(y)\n",
    "\n",
    "    # loop over unique elements of y\n",
    "    for i in np.arange(len(uniquey)): \n",
    "\n",
    "        # probability that y==y(i) (prob of each y)\n",
    "        py = np.sum(y==uniquey[i]) / N\n",
    "\n",
    "        # then loop over all possible x's to compute entropy of x at each y\n",
    "        tmp=0\n",
    "        for j in np.arange(len(uniquex)):\n",
    "            px_y = np.sum((x==uniquex[j]) & (y==uniquey[i])) / np.sum(y==uniquey[i])    # e.g. prob x==1 when y==0\n",
    "            tmp += (-( px_y * np.log2(px_y) ))                                     # entropy      \n",
    "\n",
    "        # then tally up entropy of x given each specific y multiplied by the probability of that y (py)\n",
    "        Hxy += py*tmp\n",
    "\n",
    "    return Hxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets give the functions a try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000   # number of data points\n",
    "x = np.round(np.random.rand(N))\n",
    "y = np.round(np.random.rand(N))\n",
    "Hx = entropy(x=x)\n",
    "Hxy = condEntropy(x=x,y=y)\n",
    "print('MI is: ', Hx-Hxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets try a more complicated (and useful) example where we have one discrete variable and one continuous variable\n",
    "* This often comes up in experiments where you have a set of discrete stimuli and then a continuous output measure (e.g. LFP power, spike rate over a time window, etc)\n",
    "* Logic here is the same as the case for two discrete variables, but generating the PDF for the continuous variable is trickier and there are many (many many) ways of doing it. \n",
    "    * For comparison, with our coin tosses, you can get the PDF by sum(heads)/total_tosses...\n",
    "    * But for a continuous metric where you have all unique values, this doesn't work (i.e. if each value happens once and only once then how do you compute the likelihood of observing that value?\n",
    "    * To solve this problem, we need to generate an estimate of the data generating PDF based on our samples of the continuous data\n",
    "    * Common approaches are to build histograms of the data to approximate the PDF, or to use an approach like Kernel Density Estimation to approximate the PDF \n",
    "* We'll use the KDE implementation in sklearn \n",
    "* We'll also turn the MI formula around a bit more (its symetric so this is ok)\n",
    "    * Instead of MI = Hx - Hxy, we'll use MI = Hy - Hyx\n",
    "    \n",
    "[histograms vs KDE - good read!](https://mglerner.github.io/posts/histograms-and-kernel-density-estimation-kde-2.html?p=28)\n",
    "\n",
    "[And a TMI read for muliple KDE implementations](https://jakevdp.github.io/blog/2013/12/01/kernel-density-estimation/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import KDE functionality from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up our variables - one discrete and one continuous\n",
    "* lets assume that our data came from an experiment where we show 4 different stimuli (e.g. motion directions) and then record spike rate as a continuous metric (e.g. spike rate smoothed with a 5ms Gaussian window). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of trials in our experiment\n",
    "N = 1000\n",
    "\n",
    "# Generate a list of stims\n",
    "num_stims = 4\n",
    "stims = np.repeat(np.arange(4), N/num_stims) # div by num_stims to keep output length constant\n",
    "\n",
    "# Now generate our list of continuous spike rates in response to each stimulus\n",
    "# Set it up so that there is a differential response to each of the 4 stims (i.e. MI>0)\n",
    "scale_factor = 2 # how much response to each stim differs\n",
    "resp=[]\n",
    "for i in np.arange(num_stims):\n",
    "    resp = np.hstack((resp, np.random.randn(int(N/num_stims))+(i*scale_factor)))\n",
    "    \n",
    "plt.plot(np.arange(N), resp, 'k', linewidth=2)\n",
    "plt.xlabel('Trial Number', **fig_font)\n",
    "plt.ylabel('Resp(Hz)', **fig_font)\n",
    "plt.show()\n",
    "\n",
    "# also show as a histogram to get a feel for the distribution\n",
    "num_bins = 10 # change this! you'll get a feel for why using histograms for estimating PDFs is tricky\n",
    "plt.hist(resp, bins=num_bins)\n",
    "plt.xlabel('Resp(Hz)', **fig_font)\n",
    "plt.ylabel('Trial count', **fig_font)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now use sklearn KDE algorithm to estimate a PDF of our continuous response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first define a set of points over which to evaluate the PDF\n",
    "\n",
    "# figure out min and max of our data - good start for picking a range (use ceil and floor...)\n",
    "min_resp = np.floor(np.min(resp))\n",
    "max_resp = np.ceil(np.max(resp))\n",
    "\n",
    "# then define the number of points that we want to eval the function over\n",
    "# I'll choose N here \n",
    "num_points = N\n",
    "\n",
    "# then define the x_range\n",
    "x_range = np.linspace(min_resp, max_resp, num_points)\n",
    "\n",
    "# KDE function expects a 2d array, so expand x to have a singular second dim\n",
    "if len(x_range.shape):\n",
    "    x_range = x_range[:, np.newaxis]\n",
    "    \n",
    "print(x_range.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then lets call our KDE function and we'll just pick an arbitrary bandwidth for the kernel\n",
    "bw = .2\n",
    "\n",
    "# make our data a N,1 matrix for input to KDE function\n",
    "if len(resp.shape)==1:\n",
    "    resp = resp[:,np.newaxis]\n",
    "\n",
    "kde = KernelDensity(kernel='gaussian', bandwidth=bw).fit(resp)\n",
    "log_dens = kde.score_samples(x_range)\n",
    "\n",
    "# plot\n",
    "plt.fill(x_range[:, 0], np.exp(log_dens), alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "num_bins=30\n",
    "plt.hist(resp, bins = num_bins, alpha=0.5, normed=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notice that I just selected an arbitrary kernel bandwidth to estimate the PDF\n",
    "* First go back and play with it a bit in the above cell to see what happens as you make it smaller/bigger\n",
    "* We can then make this choice in a more principled manner by using cross-validation where part of the data is held out, a kernel fit is estimated and applied to the remaining data, and this process is iterated over a bunch of kernels until the one with the best cross-validated maximum likelihood is found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# number of cross-validation folds\n",
    "cv_folds = 10\n",
    "min_bw = 0.1\n",
    "max_bw = 1.0\n",
    "bw_steps = 50\n",
    "grid = GridSearchCV(KernelDensity(),\n",
    "                    {'bandwidth': np.linspace(min_bw, max_bw, bw_steps)},\n",
    "                    cv=cv_folds) \n",
    "grid.fit(resp)\n",
    "\n",
    "best_bandwidth = grid.best_params_\n",
    "\n",
    "print(best_bandwidth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then lets call our KDE function with the best bandwidth kernel\n",
    "bw = best_bandwidth['bandwidth']\n",
    "\n",
    "# make our data a N,1 matrix for input to KDE function\n",
    "if len(resp.shape)==1:\n",
    "    resp = resp[:,np.newaxis]\n",
    "\n",
    "kde = KernelDensity(kernel='gaussian', bandwidth=bw).fit(resp)\n",
    "log_dens = kde.score_samples(x_range)\n",
    "\n",
    "# save out p(y) function\n",
    "py = np.exp(log_dens)\n",
    "\n",
    "# normalize to unit area\n",
    "py /= np.sum(py)\n",
    "\n",
    "# plot!\n",
    "plt.fill(x_range[:, 0], py, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First compute the entropy of the continuous variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hy = -sum(py * np.log2(py))  \n",
    "print(Hy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that we know how to compute an estimate of the PDF of our continuous variable we can compute MI by figuring out Hyx, or the average entropy of y at each given x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MI = Hy - Hyx\n",
    "\n",
    "# find num unique stims\n",
    "uniquex = np.unique(stims)\n",
    "\n",
    "# do a cheat here - we know that x has an equal number of 0,1,2,3... in it, so we can figure\n",
    "# out the probability of any given value by just computing it for one entry\n",
    "px = N/num_stims/N\n",
    "\n",
    "# then loop over all possible x's to compute entropy of y given each x (Hyx)\n",
    "Hyx=0\n",
    "for j in np.arange(len(uniquex)):\n",
    "    \n",
    "    # grab data from all trials where stims == j\n",
    "    y_x = resp[stims==j]\n",
    "    \n",
    "    # do KDE on this subject of data conditioned on x\n",
    "    # note: could cross-val to determine bw here as well\n",
    "    \n",
    "    # compute KDE\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=bw).fit(y_x)\n",
    "    log_dens = kde.score_samples(x_range)\n",
    "\n",
    "    # save out kde solution\n",
    "    tmp_y_x = np.exp(log_dens)\n",
    "\n",
    "    # normalize to unit area\n",
    "    tmp_y_x /= np.sum(tmp_y_x)\n",
    "    \n",
    "    # then tally up entropy of y given each specific x multiplied by the probability of that x (px)\n",
    "    Hyx += px*(-sum( tmp_y_x * np.log2(tmp_y_x) ))                                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MI = Hy - Hyx\n",
    "print(MI)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
